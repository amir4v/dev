{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum:\n",
    "\n",
    "pipeline, pipelines, ml pipelines, sklearn pipelines, ml models pipelines, pre-preocessing pipelines\n",
    "\n",
    "-\n",
    "\n",
    "Cost Function\n",
    "\n",
    "Missing Values\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "Hyper Parameters\n",
    "\n",
    "Dimension Reduction\n",
    "\n",
    "Regularization / L1/L2\n",
    "\n",
    "Scaling / Feature Scaling\n",
    "\n",
    "Underfitting / Overfitting\n",
    "\n",
    "MinMaxScaler / StandardScaler\n",
    "\n",
    "Feature Extraction / Feature Selection\n",
    "\n",
    "Mean Absolute Error (MAE) / Mean Squared Error (MSE)\n",
    "\n",
    "Fit, Transform, Column Transformer\n",
    "\n",
    "OneHotEncoding, LabelEncoder, Encoding Categorical Data\n",
    "\n",
    "Null Hypothesis, Alternative Hypothesis, H0 and H1, Error type 1 and Error type 2\n",
    "\n",
    "Gradient Descent / Batch Gradient Descent / Mini Batch Gradient Descent / Stochastic Gradient Descent\n",
    "\n",
    "Accuracy Score / R2-score / F1-score / cross val score / Precision & Recall / Cross Validate / Cross Validation / Evaluation / K Fold\n",
    "\n",
    "-\n",
    "\n",
    "Linear Regression\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "SVM (Support Vector Machine) / SVC / SVR\n",
    "\n",
    "KNN (K Nearest Neighbors) / K-Means / K Neighbors Classification / K Neighbors Regression\n",
    "\n",
    "Naive Bayes (Naive Bayes in DS and ML) / Naive Bayes Classification / Bernoulli Naive Bayes Classification / Gaussian Naive Bayes Classification\n",
    "\n",
    "- -\n",
    "\n",
    "Bagging (Bootstrap Aggregating) / Bagging Classification / Bagging Regression\n",
    "\n",
    "Random Forest / Random Forest Classification / Random Forest Regression\n",
    "\n",
    "Decision Tree / Decision Tree Classification / Decision Tree Regression\n",
    "\n",
    "Gradient Boosting / XGB/XGBoost\n",
    "\n",
    "Ensemble Learning\n",
    "\n",
    "DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Search\n",
    "(used in optimization algorithms to find an optimal step size to a way)\n",
    "# Cost Function\n",
    "(the loss function, measures the difference between actual values and predicted values)\n",
    "# Missing Values\n",
    "(how to handle them?)\n",
    "# Confusion Matrix\n",
    "(evaluates the accuracy of a classification model)\n",
    "# Hyper Parameters\n",
    "(parameters that are set before the learning/training begins)\n",
    "# Dimension Reduction\n",
    "(reduce the number of features or dimentions to simplifying without sacrificeing much predictive power)\n",
    "# Regularization / L1/L2\n",
    "(to prevent overfitting, adding a penalty to prevent this; L1 and L2 prevent overfitting; L1 regularization (Lasso) adds a penalty to the loss function and this encourages sparsity; L2 regularization (Ridge) adds a penalty to the loss function and it encourages sparsity but not as aggresvie as the L1 regularization)\n",
    "# Scaling / Feature Scaling\n",
    "(adjust the range of features to a similar scale like between 0 and 1 or a range with mean of 0 and std of 1; we can imporve the performance and stability of ml models and preventing any single feature from dominating the learning rate)\n",
    "# Underfitting / Overfitting\n",
    "(Underfitting is when a model is to simple and performs poorly not onlty on training data but also on new and unseen data and it happens when the model is too simple and basic relative to complexity of the data to capture the underlting patters of the data; Overfitting happens when the model learns the dateils and noise in the data and it negatively impacts the model's performance on new and unseen data and overfitting occurs when the models is to complex relative to the amount and noise level in the traing data; to address underfitting we can use more complex models or increase models complexity by adding more features; to address overfitting we can use techniques like regularization or cross-validation or reducing model cimplexity)\n",
    "# MinMaxScaler / Standard Scaler\n",
    "(MinMaxScaler transforms features by scaling them to a specified range typically between 0 and 1; StandardScaler standardizes features by removing the mean and scaling to unit variance which makes the data more Gaussian-like and usually it is a range with mean of 0 and std of 1)\n",
    "# SVD (Singular Value Decomposition)\n",
    "(is a fundamental matrix decomposition; it decomposes a matrix into three simpler matrices and enabling efficient computation and analysis of the original matrix's properties; applications are: dimensionality reduction, matrix approximation, matrix inversion)\n",
    "# Feature Extraction / Feature Selection\n",
    "(FeatureExtraction involves transforming raw data into a set of features that are more informative and suitable for ml algorithms; FeatureSelection involves selectinga subset of the most relevant features from the original set of features and the goal is to reduce the dimentionality of the data while retaining the most important information for modeling and it helps improve model performance, reduce overfitting and enhance interpretability; in summary, feature extraction transforms data into meaningful features, while feature selection chooses the most important features to use in ml models)\n",
    "# Mean Absolute Error / Mean Squared Error\n",
    "(Are common metrics used to evaluate the performance of regression models; MAE is a metric which measures the average magnitude of errors between predicted and actual values and it is calculated as the mean of the absolute differences between predicted and actual values; MSE is a metric which measures the average of the squared differences between predicted and actual values and it penalizes large errors more than smaller errors and is more sensitive to outliers compared to MAE)\n",
    "# Transform, fit_transform / Column Transformer\n",
    "(ColumnTransformer allows for applying different preprocessing steps to different columns while transform and fit_transform are methods used to apply transformations to data based learnd parameters using fit method)\n",
    "# OneHotEncoding / Label Encoder / Encoding Categorical Data\n",
    "(OneHotEncoding is a technique used to convert categorical variables into a binary format that can be used by ml algorithms an each category is represented as a binary vector where only one bit hot (1) indicating the presence of that category; LableEncoder is a technique used to convert categorical labels into numerical representations and each unque label is mapped to an integer value and is useful when dealing with ordinal categorical variables where the order of labels matters; EncodingCategoricalData is refers to the process of converting categorical data into numerical format)\n",
    "# Null Hypothesis / Alternative Hypothesis / H0 and H1 / Erro type 1 and 2\n",
    "(Type 1 error also known as a false positive and it occurs when you reject a true null hypothesis or H0 and it's the probability of incorrectly concluding that there is an effect difference when there actually isn't; Type 2 error also known as a false negative and it occurs when you fail to reject a false null hypothesis or H0 and it's the probability of incorrectly concluding that there is no effect or difference when there actually is; NullHypothesis/H0 is a statement that suggest there is no effect or relasionship in the population being studied and it's typically the hypothesis to be tested against an alternative hypothesis; AlternativeHypothesis/H1/Ha is a statement that contradicts the null hypothesis and suggests there is an effect or relationship in the population and it's what researchers are trying to find evidence for)\n",
    "# Gradient Descent / Batch Gradient Descent / Mini Batch Gradient Descent / Stochastic Gradient Descent\n",
    "(GradientDescent is an optimization algorithm used to minimize a function by iteratively moving the direction of the steepest descent of the function's gradient; BatchGradientDescent is a variant of gradient descent that computes the gradient using the entire dataset and it updates the model parameters after evaluating all training examples, making it slower but more accurate for convergence; MiniBatchGradientDescent is a variant of gradient descent that computes the gradient using a subset or mini-batch of the dataset and it strikes a balance between batch gradient descent and stochastic gradient descent by updating the model parameters based on batches of data which improves training efficiency; StochasticGradientDescent/SGD is a variant of gradient descent that computes the gradient using a single training example at a time and it updates the model parameters frequently and makin it faster but more susceptible to noise and fluctuations)\n",
    "# Accuracy Score / r2-score / F1-score / cross val score / Precision & Recall / Cross Validate / Evaluation / K Fold\n",
    "(AccuracyScore measures the proportion of correct predictions both true positive and true negative, made by a classifier; r2-score(Coefficient of Dtermination) measures the proportion of the variance in the dependent variable that is predictable from the independent variables and r2-score ranges from 0 to 1 where 1 indicates a perfect fit; F1-score is the harmonic mean of precision and recall, providing a balance between these two metrics and it's useful for binary classification tasks where precision is the ratio of true true positive predictions to all positive predictions and recall is the ratio of true positive predictions to all actuall positives; CrossValidScore/CrossValidationScore it evaluates a model's performance by splitting the data into multiple subsets (folds), training the model on several combinations of these subsets and then averaging the results which helps assess a model's generalization performance; K-Fold-Cross-Validation is a specific type of cross-validation where the data is divided into k subsets (folds) and the model is trained and evaluated k times and each time using a different fold for evaluating and the remaining folds for training which helps to provide a more reliable estimate of a model's performance compared to a single train-test split; Precision, is the proportion of true positive predictions (correctly predicted positive) out of all positive predictions made by a classifier and it measures the accuracy of positive predictions; Recall, also known as sensitivity or true positive rate, is the proportion of true positive predictions (correctly predicted positive) out of all actual positive instances in the data and it measures the ability of a classifier to identify all relevant instances (true positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "(Is a statistical method used to model the relation between a dependent variable and one or more indeoendent variables and the goal is to find a linear equation that best predicts the value of Y based on the values of X and the coefficients are estimated from the data to minimize the difference between predicted and actual Y values)\n",
    "# Logistic Regression\n",
    "(Is a statistical method used for binary classification tasks where the dependent variable Y is categorical with two possible outcomes and it estimates the probability that Y belongs to a particular category based on one or more independent variables, and instead of predicting the actual outcome directly, logistic regression models the log-odds of the probability of Y being in a specific category and the output of logistic regression is a probability score between 0 and 1)\n",
    "# SVM (Support Vector Machine) / SVC / SVR\n",
    "(SVM is a supervised machine learning algorithm used for both classification and regression tasks and it works by finding the hyperplane that best separates different classes in a high-dimensional space, it aims to maximize the margin between the classes which helps in achieving good generalization to new data; SVC is specially the svm algorithm used for classification tasls and it finds the optimal hyperplane that best divides the classes in the feature space and is effective for binary and multiclass classification problems; SVR is the svm algorithm used for regression tasks and instead of predicting discrete class labels, it works by finding a hyperplane that maximizes the error between predicted and actual values while still maximizing the margin)\n",
    "# KNN (K Nearest Neighbors) / K-Means / K Neighbors Classification / K Neighbors Regression\n",
    "(KNN used for classification and regression and works based on the principle of similarity where new data points are classified based on the majority class or average value of their nearest K neaighbors and requires choosing a value for K which represents the number of neighbors to consider; K-Means is an unsupervised algorithm used for clustering and divides adataset into K clusters based on similarity with each cluster represented by its centroids (center points) and works by iteratively assigning data points to the nearest cluster centroid and updating the cebtroids based on the mean of the assigen points)\n",
    "# Naive Bayes (Naive Bayes in DS and ML) / Naive Bayes Classification / Bernoulli Naive Bayes Classification / Gaussian Naive Bayes Classification\n",
    "(NaiveBayes assumes that the features are conditionally independent given the class label which simplifies the computation, and it often performs well and efficient for text classification; NaiveBayesClassification refers to using the naive bayes algorithm for classification and it calculates th eprobability of a data point belonging to each class based on the observed features and then selects the class with the highest probability as the prediction; BernouliNaiveBayesClassification a specific variant of naive bayes suited for binary feature data presence or absence of a feature, it assumes that features are binary and uses the bernouli distribution to model the liklihood of each feature given the class; GaussianNaiveBayesClassification a variant of naive bayes suitable for continuous feature data assumed to follow a gaussian/normal distribution, it assumes that features are normally distributed within each each class and estimates mean and variance and variance parameters for each feature and class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating) / Bagging Classification / Bagging Regression\n",
    "(Bagging/BotstrapAggregation involves creating multiple models using subsets of the training data and it uses a technique called bootstraping where random samples of data are repeatedly draw with replacement and the final prediction is often an average for regression or a majority vote for classification of predictions from these models and the goal is to reduce variance and improve model performance by reducing overfitting; BaggingClassification multiple classifiers like decision trees are trained on different subsets of the training data and each classifier gives a prediction and the final prediction is determind by majority voting among these classifiers; BaggingRegression multiple regression models are trained on different bootstrapped samples of the training data and the final regression output is often the average oft he outputs from these models)\n",
    "# Gradient Boosting / XGB/XGBoost\n",
    "(GradientBoosting is a ml technique used for builing predictive models particularly decision trees and it builds models in a sequential manner where each new model corrects errors made by the previous model and the key idea is to optimize a loss function by minimizing errors through an iterative process; XGB/XGBoost/ExtremeGradientBoosting is a popular implementation of gradient boosting designed to be highly efficient and scalable and it includes several enhancements over traditional gradient boosting such as regularization and parallel processing)\n",
    "# Ensemble Learning\n",
    "(is a ml technique where multiple models are combined to improve the overall performance and accuracy of the predictive model instead of relying on a single model and common ensemble methods include bagging(like random forest) and boosting (like gradient boosting) and stacking)\n",
    "# DBSCAN\n",
    "(stands for Density Based Spatial Clustering of Applications with Noise, it's a popular clustering algorithm used in machine learning and data mining, and is designed to identify clusters of points based on the density of data points in the feature space and it doesn't requires specifying the number of clusters beforehand and making it useful for datasets where the number of clusters is not know and the algorithm groups together points that are closely packed and defining clusters as regions of low density (noise) and it handle noise points and outliers effectively as they are not assigned to any specific cluster; in summery it is a density based clustering algorithm that automatically identifies clusters in data based on density and is robust to outliers and mnoise in the dataset)\n",
    "# Poission Regression\n",
    "(is a type of regression analysis used when the dependent variable is a count or an event occurrence whithin a fixed period of time or area and it models the relationship between the dependent variable (count data) and one or more independent variables assuming a poisson distribution for the dependent variable, and isused to model count data assuming a poisson distribution for the dependent variable and it's especially useful when dealing with data that represent counts of events)\n",
    "# Grid Search CV/CrossValidation\n",
    "(is a technique used in ml for hyperparameter tuning which is the process of finding the optimal hyperparameters for a model; it exhaustively searches through a specific parameter grid to find the best combination of hyperparameters for a given model; it evaluates each combination using cross-validation to assess the model's performance and prevent overfitting; helps automate the process of tuning hyperparameters and saving time and effort compared to manual tuning; this technique is commonly used to optimize parameters for models like support vector machines, decision trees, random forest and more)\n",
    "# Decision Tree / Decision Tree Classification / Decision Tree Regression\n",
    "(DecisionTree is a tree-like model used for making decisions based on features or attributes of data and it is a supervised learning algorithm that partitions the data into subsets based on featur vakues, creating a tree structure of decisions and each internal node represents a feature and each branch represents a decision rule based on that feature and each leaf node represents the outcome or class label; DecisionTreeClassification is a type of supervised learning where the goal is to predict a categorical target variable (class label) the decision tree algorithm learns rules from the training data to classify instances into different predefined classes or categories and it's efficient for both binary and multi-class classification tasks and is interpretable due to its tree-like structure; DecisionTreeRegression is a supervised learning technique used for predicting continiuous target variables (numeric values) instead of predicting class labels, predicts the value of the target variable based on the features)\n",
    "# Random Forest / Random Forest Classification / Random Forest Regression\n",
    "(RandomForest is an ensemble learning technique based ondecision trees and it creates multiple decision trees during training and combines their predictions through voting for classification or averaging for regression and the randomness comes from using bootstrap samples of the data and rendom subsets of features for training each tree; RandomForestClassification uses the random forest technique for predicting categorical outcomes (class labels) and it trains multiple decision trees on random subsets of the training data and combines their predictions through majority voting and is less prone to overfitting compared to individual decision trees; RandomForestRegression applies the random forest technique to predict continuous numerical values and it builds an ensemble of decision trees that collectievly predict the target variable and the final prediction is the average of predictions from all trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
