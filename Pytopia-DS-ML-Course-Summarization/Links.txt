https://realpython.com/learning-paths/data-visualization-python/
https://realpython.com/learning-paths/data-science-python-core-skills/
https://realpython.com/learning-paths/pandas-data-science/
https://realpython.com/tutorials/devops/
https://realpython.com/learning-paths/python-devops/
https://realpython.com/tutorials/tools/
https://realpython.com/learning-paths/perfect-your-python-development-setup/
https://realpython.com/effective-python-environment/
https://realpython.com/pypi-publish-python-package/
https://realpython.com/pyinstaller-python/
https://realpython.com/tutorials/databases/
https://realpython.com/learning-paths/data-collection-storage/
https://realpython.com/tutorials/testing/
https://realpython.com/learning-paths/test-your-python-apps/
https://realpython.com/tutorials/machine-learning/
https://realpython.com/learning-paths/machine-learning-python/
https://realpython.com/learning-paths/math-data-science/
https://realpython.com/numpy-scipy-pandas-correlation-python/

https://git-scm.com/book/en/v2/GitHub-Contributing-to-a-Project
https://docs.github.com/en/get-started/exploring-projects-on-github/contributing-to-a-project
https://gist.github.com/MarcDiethelm/7303312
https://www.dataschool.io/how-to-contribute-on-github/
https://www.freecodecamp.org/news/git-and-github-workflow-for-open-source/
https://medium.com/nebula-graph/how-to-be-a-github-contributor-in-five-minutes-72175c036c17
https://daily.dev/blog/how-to-contribute-to-open-source-github-repositories
https://javascript.plainenglish.io/how-to-contribute-to-a-github-repository-project-78f777623f18?gi=a5963083d55a
https://www.reddit.com/r/learnprogramming/comments/7hjkya/how_can_i_start_contributing_to_github_projects/
https://www.youtube.com/watch?v=b_aF5zk22cA
https://docs.github.com/en/get-started/start-your-journey/about-github-and-git
https://docs.github.com/en/actions/quickstart
https://github.com/kamranahmedse/developer-roadmap/blob/c3a61e7f3464a71e689dfe9e42ecc45fddff36c5/contributing.md
https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests

https://github.com/microsoft/ML-For-Beginners
https://microsoft.github.io/ML-For-Beginners/pdf/readme.pdf
https://github.com/microsoft/ai-for-beginners
https://github.com/microsoft/Data-Science-For-Beginners
https://github.com/microsoft/generative-ai-for-beginners/tree/main?WT.mc_id=academic-105485-koreyst
https://github.com/microsoft/generative-ai-for-beginners
https://github.com/microsoft/AI-For-Beginners/blob/main/etc/pdf/readme.pdf

https://interviewsby.ai/

https://relocationjobs.org/

.

Data Structures and Algorithms
Test/UnitTest/PyTest
PyTest
Git rebase
Git
Git cherry-pick
To add GitHub SSH (Secure Shell) authentication to your Git configuration
SQL
NoSQL
API
RPC/gRPC
Design Patterns
SOLID
REST
Python
Django
FastAPI
DB
Agile-Scrum-Sprint-Daily
Clean Code
Documentation
Contribution to git and github and projects and open-source
Git-Merge-PullRequest-MergeRequest
CI/CD
GitHubActions-Workflow
Microservices
OOP
ACID
SEMAT
RAD
TDD-MDD-DDD
NVC-MVP-MVVM
Redis
Docker
-untilAlgebra-Trig-Equations-PreCalc-
Limit-Derivative-Integral
LinearAlgebra
Statistics&Probability
PreProcessing
Models
Best Practices
ML Algorithms in details
ML Models in details
DS and ML concepts and details
-Interview-
Interview General Questions
Interview HR Questions
Interview Technical Questions
Interview SoftSkills
Interview Specified Preparation for them
Before the first Interview you have to have a very good project/portfolio

---> ---> --->

Data Structures and Algorithms
Test/UnitTest/PyTest
API/REST
Microservices
Git
    CI/CD
    Git rebase
    Contribution
    GitHubActions-Workflow
    Git-Merge-PullRequest-MergeRequest
SE
    Agile-Scrum-Sprint-Daily
    Clean Code
    Design Patterns
    SOLID
    Documentation
    OOP
    ACID
    SEMAT
    RAD
    TDD-MDD-DDD
    NVC-MVP-MVVM
-untilAlgebra-Trig-Equations-PreCalc-
    Limit-Derivative-Integral
    Statistics&Probability
    LinearAlgebra
DS and ML concepts and details
    Models
    PreProcessing
    Best Practices
    ML Models in details
    ML Algorithms in details
-Interview-
    Interview General Questions
    Interview HR Questions
    Interview Technical Questions
    Interview SoftSkills
    Interview Specified Preparation for them

---> ---> --->

Test/UnitTest/PyTest
    PyTest:
        .
    Understanding different types of software testing is essential for ensuring the quality and reliability of software systems. Let's define and differentiate various types of software testing commonly used in the software development lifecycle:
    1. Unit Testing:
        Definition: Unit testing is the process of testing individual units or components of a software application in isolation.
        Purpose: To validate that each unit (e.g., function, method, class) performs as expected and meets its functional requirements.
        Scope: Tests focus on a specific piece of code and are typically automated using testing frameworks like unittest (for Python).
    2. Component Testing:
        Definition: Component testing involves testing individual components or modules of an application to verify their interactions and interfaces.
        Purpose: To ensure that the components function correctly when integrated into the larger system.
        Scope: Tests cover multiple units or related units to validate component-level behavior and interactions.
    3. Integration Testing:
        Definition: Integration testing is the process of testing the integration or interaction between different units, modules, or systems.
        Purpose: To identify and address issues related to communication, data exchange, and interoperability between integrated components.
        Scope: Tests focus on interfaces and interactions between components, ensuring that integrated units work together as expected.
    4. End-to-End Testing:
        Definition: End-to-end testing involves testing the entire software application from start to finish to simulate real-world user scenarios.
        Purpose: To validate the complete flow of an application and ensure that all components and systems work together seamlessly.
        Scope: Tests cover all layers of the application (frontend, backend, database) to verify functionality and performance across the entire system.
    5. Manual Testing:
        Definition: Manual testing is the process of manually executing test cases without using automated testing tools or scripts.
        Purpose: To evaluate software applications from a user's perspective and identify user experience (UX) issues.
        Scope: Tests involve human testers performing exploratory testing, usability testing, and user acceptance testing (UAT).
    6. Performance Testing:
        Definition: Performance testing involves evaluating the performance characteristics of a software application under specific load conditions.
        Purpose: To assess response time, throughput, scalability, and resource usage to identify performance bottlenecks and optimize system performance.
        Scope: Tests simulate real-world usage scenarios to measure and analyze the behavior of the application under different load levels.
    7. Security Testing:
        Definition: Security testing is the process of identifying vulnerabilities and ensuring the security of a software application.
        Purpose: To assess the security posture of the application and mitigate potential security risks and threats.
        Scope: Tests focus on evaluating security controls, authentication mechanisms, access controls, and data protection measures.
    8. Regression Testing:
        Definition: Regression testing involves re-running previously executed test cases to ensure that recent code changes have not adversely affected existing functionalities.
        Purpose: To validate that new code changes do not introduce new bugs or regressions into the software application.
        Scope: Tests cover critical functionalities and edge cases to detect unintended side effects caused by software updates or modifications.
    Each type of software testing plays a crucial role in ensuring the overall quality, reliability, and security of software applications. It's essential to incorporate a combination of these testing approaches into the software development process to identify and address issues at different stages of development and deployment.
Git/Contribution/CI-CD
    Git:
        . To add GitHub SSH (Secure Shell) authentication to your Git configuration:
            # Generate SSH key pair (if you don't have one)
                ssh-keygen -t rsa -b 4096 -C "your_email@example.com"
            # Start SSH agent (if not already running)
                eval "$(ssh-agent -s)"
            # Add SSH private key to SSH agent
                ssh-add ~/.ssh/id_rsa
            # Copy SSH public key to clipboard
                pbcopy < ~/.ssh/id_rsa.pub
                cd /path/to/your/repository
            # Change Git remote URL to use SSH
                git remote set-url origin git@github.com:<username>/<repository>.git
            # Test SSH connection to GitHub
                ssh -T git@github.com
            # Push changes to remote repository
                git push origin <branch_name>
            # Pull changes from remote repository
                git pull origin <branch_name>
            # Additional Notes:
                Ensure that the SSH URL (git@github.com:<username>/<repository>.git) matches your GitHub repository URL.
                If you encounter permission issues, ensure that your SSH key is correctly added to GitHub and that the remote URL is updated to use SSH.
                Use ssh-keygen -l -f ~/.ssh/id_rsa.pub to display the fingerprint of your SSH public key for verification.
        1. Initializing a Repository:
            git init: Initialize a new Git repository in the current directory.
        2. Cloning a Repository:
            git clone <repository_url>: Clone a remote repository to your local machine.
        3. Basic Configuration:
            git config --global user.name "<your_name>": Set your username globally.
            git config --global user.email "<your_email>": Set your email globally.
        4. Checking Repository Status:
            git status: Show the current state of the repository (tracked/untracked files, changes, etc.).
        5. Adding Changes:
            git add <file_name>: Add specific file changes to the staging area.
            git add . or git add -A: Add all changes (including new, modified, and deleted files) to the staging area.
        6. Committing Changes:
            git commit -m "commit_message": Commit staged changes with a descriptive commit message.
        7. Viewing Commit History:
            git log: View commit history (use q to exit log view).
            git log --oneline: View compact commit history (one line per commit).
        8. Branching:
            git branch: List all local branches.
            git branch <branch_name>: Create a new branch.
            git checkout <branch_name>: Switch to an existing branch.
            git checkout -b <new_branch_name>: Create and switch to a new branch.
        9. Merging Branches:
            git merge <branch_name>: Merge changes from another branch into the current branch.
        10. Updating and Synchronizing:
            git pull: Fetch and merge changes from the remote repository to the local branch.
            git push: Push local commits to the remote repository.
        11. Resolving Conflicts:
            git diff: Show differences between the working directory, staging area, and last commit.
            Resolve conflicts manually in the files and then stage changes with git add followed by git commit.
        12. Working with Remotes:
            git remote -v: List all remote repositories (origin, upstream, etc.).
            git remote add <name> <url>: Add a new remote repository.
        13. Undoing Changes:
            git checkout -- <file_name>: Discard local changes in a specific file.
            git reset --hard HEAD: Reset all changes in the working directory to the last committed state.
        14. Tagging Releases:
            git tag <tag_name>: Create a lightweight tag for the current commit.
            git tag -a <tag_name> -m "tag_message": Create an annotated tag with a message.
        15. Additional Commands:
            git fetch: Fetch changes from a remote repository without merging.
            git diff <commit_id>: Show differences between the working directory and a specific commit.
        16. Stashing Changes:
            git stash: Temporarily save (or "stash") local changes to a stack and revert the working directory to the last commit state.
            git stash list: List all stashed changes.
            git stash apply: Apply the most recent stash and keep it in the stash stack.
            git stash pop: Apply the most recent stash and remove it from the stash stack.
        17. Rebasing:
            git rebase <branch_name>: Reapply commits from the current branch on top of another branch.
            git rebase --interactive <base_branch>: Interactive rebase to squash, reword, or reorder commits.
        18. Cherry-picking Commits:
            git cherry-pick <commit_id>: Apply a specific commit from one branch to another.
        19. Inspecting Changes:
            git show <commit_id>: Show detailed information about a specific commit.
        20. Removing Files:
            git rm <file_name>: Remove a file from the repository (also stages the deletion).
            git rm --cached <file_name>: Remove a file from version control but keep it locally.
        21. Ignoring Files:
            Create a .gitignore file: Specify patterns for files and directories to be ignored by Git.
        22. Viewing Differences:
            git diff <commit_id1> <commit_id2>: Show differences between two commits.
            git diff --staged: Show differences between the staged changes and the last commit.
        23. Branch Management:
            git branch -d <branch_name>: Delete a local branch (use -D for force deletion).
            git push origin --delete <branch_name>: Delete a remote branch.
        24. Managing Tags:
            git tag -d <tag_name>: Delete a local tag.
            git push origin --delete tag <tag_name>: Delete a remote tag.
        25. Inspecting Remote Information:
            git remote show <remote_name>: Show detailed information about a specific remote repository.
        26. Amending Commits:
            git commit --amend: Amend the last commit by modifying the commit message or adding changes.
        27. Blame/Annotate:
            git blame <file_name>: Show who last modified each line of a file and when.
        28. Customizing Git:
            .gitconfig: Configure global settings for Git using a configuration file.
        29. Inspecting Git Objects:
            git cat-file -p <object_id>: Display the contents of a Git object (commit, tree, blob).
        30. Logging and Reflogs:
            git reflog: Show a log of all Git actions (useful for recovering lost commits).
    Contribution:
        1. Forking a Repository:
        Fork the repository you want to contribute to by clicking the "Fork" button on GitHub. This creates a copy of the repository under your GitHub account.
        2. Cloning the Forked Repository:
        Clone the forked repository to your local machine using Git:
        git clone https://github.com/your-username/repository.git
        cd repository
        3. Creating a Branch:
        Create a new branch for your changes (e.g., a new feature or bug fix):
        git checkout -b feature-name
        4. Making Changes:
        Make your desired changes (add/edit files, update code, etc.) using your preferred editor or IDE.
        5. Committing Changes:
        Stage your changes for commit:
        git add .
        Commit your changes with a descriptive commit message:
        git commit -m "Brief description of the changes"
        Commit Message Best Practices:
        Keep commit messages concise, clear, and descriptive.
        Use present tense ("Add feature" instead of "Added feature").
        Provide context and motivation for the change in the commit message body (if needed).
        6. Pushing Changes:
        Push your changes to your forked repository:
        git push origin feature-name
        7. Creating an Issue:
        Create an issue on the original repository to report bugs or suggest improvements:
        Click on "Issues" in the repository menu.
        Click on "New issue" and fill in the details.
        8. Participating in an Issue:
        Comment on existing issues to provide feedback, ask questions, or offer assistance.
        Reference issues in your commit messages (e.g., Fixes #123) to automatically close issues when your PR is merged.
        9. Submitting a Pull Request (PR):
        Open a PR from your forked repository to the original repository:
        Go to your forked repository on GitHub.
        Click on "Pull requests" and then "New pull request."
        Choose the base branch (usually main or master) and compare it with your feature branch.
        Provide a descriptive title and summary for your PR.
        10. Reviewing and Addressing Feedback:
        Respond to any feedback or review comments on your PR.
        Make necessary changes based on the feedback and push new commits to your branch.
        11. Merging Changes:
        After your PR is approved, it can be merged into the base branch (e.g., main or master) of the original repository.
        Branching Best Practices:
        Use meaningful branch names that reflect the purpose of your changes (feature/add-new-feature).
        Keep each branch focused on a specific task or issue to facilitate easier review and collaboration.
        Regularly update your branch with the latest changes from the base branch (main or master) to resolve conflicts early.
        . General Best Practices:
        Follow the repository's contribution guidelines and coding standards.
        Communicate openly with maintainers and contributors through comments and discussions.
        Be respectful and courteous in all interactions.
        Continuously test your changes locally before pushing them.
        Use meaningful commit messages and atomic commits (one change per commit).
    CI-CD:
        1. Understanding Workflows
        Workflows are defined using YAML files stored in the .github/workflows directory of your repository.
        Each workflow is triggered by specific events (e.g., push, pull request, issue comment) and runs a series of steps on a virtual machine (runner) hosted by GitHub.
        2. Creating a Workflow
        Create a new YAML file (e.g., main.yml) under .github/workflows in your repository.
        Define the workflow structure including event triggers, jobs, and steps.
        3. Workflow Syntax
        Define the workflow using YAML syntax. The structure includes:
        Name: Name of the workflow.
        on: Event triggers (e.g., push, pull_request).
        jobs: Specify one or more jobs to run as part of the workflow.
        Each job can consist of multiple steps.
        Use predefined actions or custom shell commands in steps.
        runs-on: Specifies the type of runner (e.g., Ubuntu, macOS, Windows) for executing the job.
        4. Example Workflow, yaml
            name: CI Workflow
            on:
            push:
                branches:
                - main
            pull_request:
                branches:
                - main
            jobs:
            build:
                runs-on: ubuntu-latest
                steps:
                - name: Checkout code
                    uses: actions/checkout@v2
                - name: Setup Node.js
                    uses: actions/setup-node@v2
                    with:
                    node-version: '14'
                - name: Install dependencies
                    run: npm install
                - name: Run tests
                    run: npm test
        5. Using Actions
        GitHub Actions provides reusable actions that you can include in your workflows.
        Use the uses keyword in your steps to reference actions (e.g., actions/checkout@v2).
        6. Workflow Triggers
        Define event triggers (on keyword) to specify when the workflow should run (e.g., on push, pull request, schedule).
        7. Job Execution
        Use runs-on to specify the type of runner environment for job execution (e.g., Ubuntu, macOS, Windows).
        Each job runs independently on a separate virtual machine.
        8. Workflow Logs
        View workflow execution logs and status in the Actions tab of your GitHub repository.
        Debug failed workflows by inspecting logs and job output.
        9. Workflow Artifacts
        Capture and store workflow artifacts (e.g., build artifacts, test results) for later use or analysis.
        10. Workflow Security
        Use secrets and environment variables to securely store sensitive information (e.g., API keys, tokens).
        Restrict workflow execution based on branches or pull request status.
        11. Workflow Status
        Monitor workflow status using GitHub's visual indicators (checks, statuses) displayed on pull requests and commit histories.
        12. Customizing Workflows
        Customize workflows based on your project requirements using a wide range of available actions and customization options.
        13. Documentation
        Refer to GitHub Actions documentation for detailed information and examples: GitHub Actions Documentation / https://docs.github.com/en/actions
        . Structure:
            '.github'
                file.yml
                    many parts: Events, Jobs, Runners, Steps, Actions
                    Events: "on: push"
                    Actions are inside Steps
                    Runners: "runs-ob: ubuntu-latest"
                    Jobs includes: Runnsers, Steps, Actions
SE / Design Patterns / SOLID / OOP / ACID / SEMAT / RAD / TDD-MDD-DDD / MVC-MVP-MVVM
    Design Patterns:
        Creational Patterns
            Singleton: Ensures a class has only one instance and provides a global point of access to it.
            Factory Method: Defines an interface for creating an object, but lets subclasses decide which class to instantiate.
            Abstract Factory: Provides an interface for creating families of related or dependent objects without specifying their concrete classes.
            Builder: Separates the construction of a complex object from its representation, allowing the same construction process to create different representations.
            Prototype: Specifies the kinds of objects to create using a prototypical instance, which is cloned to produce new objects.
        Structural Patterns
            Adapter: Allows incompatible interfaces to work together by providing a wrapper that converts one interface into another.
            Bridge: Decouples an abstraction from its implementation so that the two can vary independently.
            Composite: Composes objects into tree structures to represent part-whole hierarchies. Clients can treat individual objects and compositions uniformly.
            Decorator: Adds behavior or responsibilities to objects dynamically, without altering their structure.
            Facade: Provides a unified interface to a set of interfaces in a subsystem, simplifying interactions for clients.
            Flyweight: Minimizes memory usage by sharing data with similar objects.
        Behavioral Patterns
            Template Method: Defines the skeleton of an algorithm in a method, deferring some steps to subclasses.
            Chain of Responsibility: Allows multiple objects to handle a request without the sender needing to specify the recipient explicitly.
            Command: Encapsulates a request as an object, allowing for parameterization of clients with different requests, queuing, logging, etc.
            Interpreter: Defines a grammar for interpreting sentences in a language and provides an interpreter to evaluate those sentences.
            Iterator: Provides a way to access elements of an aggregate object sequentially without exposing its underlying representation.
            Mediator: Defines an object that encapsulates how a set of objects interact. Promotes loose coupling by keeping objects from referring to each other explicitly.
            Memento: Captures and externalizes an object's internal state so that it can be restored later, without violating encapsulation.
            Observer: Defines a one-to-many dependency between objects, where changes in one object trigger updates in dependent objects.
            State: Allows an object to alter its behavior when its internal state changes. The object appears to change its class.
            Strategy: Defines a family of algorithms, encapsulates each one, and makes them interchangeable. Lets the algorithm vary independently from clients that use it.
            Visitor: Separates an algorithm from an object structure it operates on. It allows adding new operations to an object structure without modifying the objects themselves.
            Proxy: Provides a surrogate or placeholder for another object to control access to it.
    SOLID:
        "SOLID" is an acronym that represents a set of five principles in object-oriented programming and software design. These principles, introduced by Robert C. Martin (also known as Uncle Bob), aim to guide developers in writing more maintainable, flexible, and understandable code. Each letter in "SOLID" stands for a different principle:
        1. Single Responsibility Principle (SRP)
            A class should have only one reason to change, meaning it should have only one job or responsibility.
            This principle promotes high cohesion by ensuring that a class is focused on doing one thing well.
            Benefits include easier maintenance, reduced coupling, and increased reusability.
        2. Open/Closed Principle (OCP)
            Software entities (classes, modules, functions) should be open for extension but closed for modification.
            This principle encourages using abstraction and polymorphism to allow behavior to be extended without modifying existing code.
            Benefits include code stability, easier maintenance, and scalability.
        3. Liskov Substitution Principle (LSP)
            Objects of a superclass should be replaceable with objects of its subclasses without affecting the correctness of the program.
            This principle ensures that subtypes adhere to the behavior expected of the supertype, preventing unexpected side effects or errors.
            Benefits include improved modularity, flexibility, and maintainability.
        4. Interface Segregation Principle (ISP)
            Clients should not be forced to depend on interfaces they do not use.
            This principle advocates for designing fine-grained, specific interfaces tailored to the needs of clients.
            It prevents interface pollution and minimizes dependencies, leading to more modular and maintainable code.
        5. Dependency Inversion Principle (DIP)
            High-level modules should not depend on low-level modules. Both should depend on abstractions (e.g., interfaces).
            Abstractions should not depend on details; details should depend on abstractions.
            This principle encourages decoupling between components by relying on abstractions, enabling flexibility and easier testing.
        By adhering to these SOLID principles, developers can create software systems that are easier to understand, maintain, extend, and refactor. Each principle emphasizes fundamental concepts of object-oriented design, such as encapsulation, abstraction, and modularity, leading to more robust and scalable applications.
    OOP:
        Object-Oriented Programming (OOP) is a programming paradigm based on the concept of "objects," which are instances of classes. OOP focuses on organizing code into reusable and modular components, allowing for better code organization, abstraction, and encapsulation of data and behavior. Here are key concepts and principles of Object-Oriented Programming:
        1. Class:
        A class is a blueprint for creating objects. It defines the properties (attributes) and behaviors (methods) that objects of the class will have.
        2. Object:
        An object is an instance of a class. It represents a specific entity with its own state (attributes) and behavior (methods).
        3. Encapsulation:
        Encapsulation is the bundling of data (attributes) and methods (functions) that operate on the data into a single unit (class). It hides the internal state of objects and restricts direct access to certain components, promoting data security and abstraction.
        4. Abstraction:
        Abstraction focuses on exposing only the essential features of an object while hiding the complex implementation details. It allows us to work with high-level concepts without worrying about low-level details.
        5. Inheritance:
        Inheritance is the mechanism by which a class can inherit properties and behaviors from another class. It promotes code reusability and establishes an "is-a" relationship between classes.
        6. Polymorphism:
        Polymorphism allows methods to be defined in multiple classes with the same name but different implementations. It enables flexibility in method invocation based on the object's type or class hierarchy.
        7. Class Relationships:
        Composition: Combining simpler classes to create more complex ones. It involves creating instances of other classes within a class.
        Aggregation: A form of association where one class contains references to other classes as part of its state.
        . Key Principles of OOP:
        Encapsulation: Bundling data and methods that operate on the data into a single unit (class).
        Abstraction: Exposing only essential features of an object while hiding implementation details.
        Inheritance: Reusing code and establishing hierarchical relationships between classes.
        Polymorphism: Providing a way to perform a single action in different ways based on the object's type or class hierarchy.
        DRY (Don't Repeat Yourself): Avoiding code duplication by promoting code reuse through inheritance and composition.
        . SOLID Principles:
            Single Responsibility Principle (SRP): A class should have only one reason to change.
            Open/Closed Principle (OCP): Software entities should be open for extension but closed for modification.
            Liskov Substitution Principle (LSP): Objects of a superclass should be replaceable with objects of its subclasses without affecting the correctness of the program.
            Interface Segregation Principle (ISP): Clients should not be forced to depend on interfaces they do not use.
            Dependency Inversion Principle (DIP): High-level modules should not depend on low-level modules; both should depend on abstractions.
        Object-Oriented Programming provides a modular and structured approach to software development, facilitating code organization, maintenance, and scalability. Understanding and applying OOP concepts and principles can lead to more efficient and maintainable software systems.
    ACID:
        In the context of databases, ACID stands for Atomicity, Consistency, Isolation, and Durability. These are the key properties that ensure reliable and predictable transaction processing. Let's break down each component of ACID:
        1. Atomicity:
        Atomicity ensures that a transaction is treated as a single unit of work. Either all the operations within the transaction succeed and are committed, or none of them are executed at all (rolled back to a previous state). This property ensures that the database remains in a consistent state in case of failures or interruptions during transaction processing.
        2. Consistency:
        Consistency guarantees that a transaction brings the database from one consistent state to another consistent state. This means that all constraints, rules, and relationships defined in the database schema are preserved before and after the transaction. In other words, the integrity of the data is maintained.
        3. Isolation:
        Isolation ensures that the concurrent execution of multiple transactions results in a system state that would be obtained if the transactions were executed sequentially (one after the other). This property prevents interference between concurrent transactions, maintaining data integrity and preventing issues like "lost updates" or "dirty reads."
        4. Durability:
        Durability guarantees that once a transaction is committed (its changes are saved to the database), the changes persist even in the event of system failures (such as power outages, crashes, or hardware failures). The system ensures that the committed changes are permanently stored and remain intact, making them resilient to any subsequent failures.
        . In summary, ACID properties provide a robust framework for transaction management in databases, ensuring reliability, data integrity, and resilience against failures. These properties are fundamental for maintaining the correctness and consistency of data in modern database systems.
    SEMAT:
        . SEMAT, short for "Software Engineering Method and Theory," is an initiative launched in 2009 by a group of prominent software engineering researchers and practitioners with the goal of refocusing and advancing the discipline of software engineering. The initiative aims to define a foundational theory of software engineering based on a set of essential principles and practices.
        . The SEMAT initiative introduced the "Essence" framework, which is designed to provide a common language and foundation for software engineering. The Essence framework consists of a kernel and a set of extensions, enabling teams to describe and analyze software development practices in a systematic and standardized manner.
        Key components of the SEMAT initiative include:
        1. Essence Kernel:
        The Essence kernel defines a minimal set of elements (e.g., states, alphas, activities, and competencies) that are fundamental to all software development endeavors. These elements provide a common vocabulary for describing and evaluating software engineering practices.
        2. Essence Extensions:
        Essence extensions allow for customization and specialization of the kernel to accommodate specific domains, contexts, or methodologies within software engineering. Extensions can be used to incorporate additional practices, tools, or techniques into the Essence framework.
        3. Essence Checklists:
        Essence checklists provide practical guidelines and criteria for assessing the completeness and maturity of software development projects. Checklists help teams evaluate their progress across different aspects of software development and identify areas for improvement.
        4. Essence Alphas and States:
        Alphas represent key artifacts or states of software development, such as requirements, architecture, implementation, and testing. States describe the maturity and progress of each alpha throughout the development lifecycle.
        5. Essence Practices:
        SEMAT promotes a set of common software engineering practices that are aligned with the Essence framework. These practices emphasize collaboration, communication, iteration, and continuous improvement in software development projects.
        . Overall, SEMAT and the Essence framework aim to promote a more rigorous and systematic approach to software engineering, fostering better communication, understanding, and management of software development activities across different teams and organizations. The initiative encourages practitioners to adopt and tailor the Essence framework to fit their specific needs and contexts, driving continuous evolution and improvement in the field of software engineering.
    RAD:
        . RAD stands for Rapid Application Development, which is a software development methodology focused on quickly building working prototypes and iterations of software applications. RAD emphasizes iterative development, user feedback, and rapid prototyping to accelerate the delivery of functional software systems. Here are key characteristics and principles of RAD:
        - Key Characteristics of RAD:
        Iterative Development:
            RAD promotes iterative cycles of development where software is developed and refined through successive iterations or prototypes. Each iteration adds new functionality based on user feedback and requirements.
        User Involvement:
            RAD emphasizes active involvement of users and stakeholders throughout the development process. User feedback is gathered early and frequently to ensure that the software meets user needs and expectations.
        Time-Bounded Development:
            RAD projects have strict time constraints, aiming to deliver working software within short development cycles (e.g., weeks or months). This time-boxed approach encourages quick turnaround and responsiveness to changing requirements.
        Prototyping:
            Prototyping is a central technique in RAD, allowing developers to quickly create visual representations of software features and functionalities. Prototypes serve as a basis for discussions and validation with stakeholders.
        Reusable Components:
            RAD encourages the use of pre-built components and libraries to accelerate development and reduce redundancy. Reusable components can be integrated and customized to meet specific project requirements.
        Close Collaboration:
            RAD fosters close collaboration among cross-functional teams, including developers, designers, testers, and business stakeholders. Collaboration helps streamline decision-making and accelerate development cycles.
        - RAD Phases:
        Requirements Planning:
            Identify and prioritize user requirements and project goals. Define the scope and objectives of the RAD project.
        Quick Design and Prototyping:
            Create rapid prototypes and mockups based on initial requirements. Gather feedback from users to refine designs and iterate on prototypes.
        Construction:
            Develop and integrate software components to build functional modules. Focus on rapid implementation and incremental delivery of features.
        Testing and Iteration:
            Conduct continuous testing and validation throughout the development process. Gather user feedback to identify issues and make necessary improvements.
        Deployment and Feedback:
            Deploy the software to production or staging environments. Gather feedback from users and stakeholders to assess usability and performance.
        - Benefits of RAD:
            Faster Time to Market: Rapid development cycles enable quicker delivery of software solutions.
            Adaptability to Change: Iterative approach allows flexibility to accommodate changing requirements.
            User-Centric Design: Early and frequent user feedback ensures alignment with user needs.
            Reduced Development Costs: Reuse of components and efficient development practices lower costs.
        . RAD is particularly effective for projects where requirements are volatile or where a quick prototype is needed to validate ideas and gather feedback. However, RAD may not be suitable for projects with stringent regulatory requirements or complex architectures that require extensive planning and design upfront. Choosing the right development methodology depends on project goals, constraints, and stakeholder preferences.
    TDD-MDD-DDD:
        . Data-Driven Design
            Definition: Data-driven design focuses on designing systems where the structure and behavior of the application are primarily determined by the underlying data model or data flow.
            Key Concepts:
            Emphasizes modeling data structures and relationships first.
            Uses data schemas, database designs, and data flows to drive application architecture.
            Common in database-centric applications or systems processing large amounts of data.
            Example: Designing an e-commerce platform where the database schema (e.g., for products, orders, customers) drives the architecture of the backend services and APIs.
        . Test-Driven Development (TDD)
            Definition: Test-driven development is a software development process that emphasizes writing tests before writing the actual implementation code.
            Key Concepts:
            Developers write automated tests that define desired behaviors.
            Tests initially fail because the functionality doesn't exist yet.
            Developers then implement code to make the tests pass.
            Helps ensure that code is written to meet specific requirements and is testable from the start.
            Example: Writing unit tests for a function's behavior (e.g., edge cases, expected outputs) before writing the function itself.
        . Behavior-Driven Development (BDD)
            Definition: Behavior-driven development is an extension of TDD that focuses on defining the behavior of software through examples in natural language.
            Key Concepts:
            Uses a common language (e.g., Gherkin syntax) to describe scenarios and expected behaviors.
            Encourages collaboration between developers, QA, and business stakeholders to define requirements.
            Helps ensure that development efforts align with business expectations and user needs.
            Example: Writing executable specifications using Given-When-Then scenarios to describe how a feature should behave.
        . Domain-Driven Design (DDD)
            Definition: Domain-driven design is an approach to software development that focuses on understanding and modeling the domain of the problem space.
            Key Concepts:
            Emphasizes a shared understanding of the domain model (e.g., entities, aggregates, value objects).
            Encourages using a ubiquitous language that reflects domain terminology.
            Guides developers to focus on core domain logic and use bounded contexts to manage complexity.
            Example: Modeling an e-commerce application by identifying core business entities (e.g., Order, Product) and their relationships.
        . Event-Driven Design (EDD)
            Definition: Event-driven design focuses on building systems where the flow of information and interactions between components are driven by events.
            Key Concepts:
            Components communicate asynchronously through events (e.g., messages, notifications).
            Promotes loose coupling and scalability by decoupling producers and consumers of events.
            Common in microservices architectures and reactive systems.
            Example: Using an event bus to propagate state changes (e.g., order placed, payment received) across different services in a distributed system.
        TDD:
            Test-Driven Development (TDD) is a software development technique where developers write automated test cases before writing the actual implementation code. TDD follows a specific cycle known as the "Red-Green-Refactor" cycle, which involves writing failing tests first, implementing the code to make the tests pass, and then refactoring the code while ensuring that the tests continue to pass. Let's break down TDD into its key concepts and steps:
            Key Concepts of TDD:
                Red-Green-Refactor Cycle:
                    Red: Write a test case that initially fails (represented by a red test result).
                    Green: Write the minimum amount of code necessary to make the test pass (turn the failing test into a passing test, represented by a green test result).
                    Refactor: Improve the code without changing its behavior to enhance its design, readability, or performance, while ensuring that all tests still pass.
                Benefits of TDD:
                    Reliable Tests: Ensures that the code is thoroughly tested against expected behavior.
                    Improved Design: Promotes writing modular, loosely coupled, and more maintainable code.
                    Confidence in Changes: Provides confidence to make changes and refactor existing code without fear of breaking functionality.
                    Faster Debugging: Helps in identifying and fixing issues early in the development process.
                    Living Documentation: Tests serve as documentation of the expected behavior of the code.
            Steps to Practice TDD:
                Write a Failing Test (Red):
                    Start by identifying a specific behavior or requirement that needs to be implemented.
                    Write a test case (in the form of a unit test) that checks for this behavior. The test should initially fail because the functionality does not exist yet.
                Implement the Minimum Code to Pass (Green):
                    Write the simplest implementation code that makes the failing test pass.
                    Focus on making the test pass without worrying about optimization or additional features.
                Refactor (Refactor):
                    Once the test is passing, refactor the code to improve its design, readability, or performance.
                    Ensure that all tests continue to pass after refactoring. Refactoring should not change the behavior of the code.
                Repeat:
                    Repeat the cycle by adding more test cases for new behaviors or edge cases.
                    Each test case should follow the Red-Green-Refactor cycle, ensuring incremental development based on specific requirements.
        MDD:
            Model-Driven Development (MDD) is an approach to software development that emphasizes the creation of models to represent the system's structure, behavior, and architecture throughout the development lifecycle. These models serve as the primary artifacts from which the software system is constructed, analyzed, and validated. Model-Driven Development aims to improve productivity, maintainability, and quality by focusing on visual representations of the system and generating executable code from these models.
            Key Concepts and Principles of Model-Driven Development:
                . Domain-Specific Modeling Languages (DSMLs):
                DSMLs are tailored modeling languages designed to express concepts and relationships specific to a particular problem domain.
                Examples include UML (Unified Modeling Language), BPMN (Business Process Model and Notation), and DSLs (Domain-Specific Languages).
                . Model-Driven Architecture (MDA):
                MDA is a framework for software development that promotes using models as primary artifacts for design, implementation, and deployment.
                It separates platform-independent models (PIMs) from platform-specific models (PSMs) to enable model transformations for code generation.
                . Model-Driven Engineering (MDE):
                MDE encompasses the use of models, metamodels, and transformations to automate software development activities.
                It involves defining domain-specific models, creating tools to manipulate these models, and generating executable code from models.
                . Automated Code Generation:
                MDD emphasizes automating the generation of executable code from models to reduce manual coding efforts and ensure consistency between models and code.
                Code generators translate high-level models into lower-level implementation details (e.g., source code in a programming language).
                . Model Transformations:
                Model transformations define rules and mappings to transform models from one representation to another.
                Transformations can be used to refine models, optimize code, or adapt models to specific platforms or technologies.
                . Tool Support:
                MDD relies on specialized modeling tools and frameworks that facilitate model creation, manipulation, validation, and code generation.
                These tools often provide graphical editors, validation checks, and transformation engines.
            Steps in Model-Driven Development:
                . Requirements Analysis and Domain Modeling:
                Identify domain concepts, relationships, and requirements that will be modeled.
                Define domain-specific modeling languages (DSMLs) if needed.
                . Create Platform-Independent Models (PIMs):
                Develop high-level models that abstractly represent system functionalities, behavior, and structure using DSMLs like UML.
                . Transform PIMs into Platform-Specific Models (PSMs):
                Refine PIMs into platform-specific models targeting specific technologies, frameworks, or implementation platforms.
                . Automate Code Generation:
                Use model transformations and code generators to produce executable code (e.g., source code, configuration files) from PSMs.
                . Validate and Iterate:
                Validate models for correctness, completeness, and consistency with requirements.
                Iterate on models based on feedback and changes in requirements.
            Benefits of Model-Driven Development:
                Increased Productivity: Automation of repetitive tasks reduces manual effort and speeds up development.
                Improved Maintainability: Models provide a structured view of the system, making it easier to understand and modify.
                Consistency and Reusability: Models promote reuse of design patterns, components, and best practices.
                Early Validation: Models can be analyzed and validated before code generation, reducing the risk of errors in implementation.
                Platform Independence: Separation of concerns between PIMs and PSMs allows targeting different platforms and technologies.
                Model-Driven Development is particularly useful for complex systems with well-defined domain concepts and requirements. It helps bridge the gap between requirements analysis and implementation by providing a visual and structured representation of the software system. However, MDD requires expertise in modeling languages, tools, and transformations to effectively leverage its benefits.
        DDD:
            "DDD" stands for Domain-Driven Design, which is an approach to software development that focuses on understanding and modeling the domain of the problem space. DDD was introduced by Eric Evans in his book "Domain-Driven Design: Tackling Complexity in the Heart of Software."
            - Key Concepts of DDD:
            . Ubiquitous Language:
            Encourages using a common language (shared by domain experts and developers) to describe the domain model.
            The language used in the code should reflect the terminology used in the domain.
            . Bounded Context:
            Defines explicit boundaries within which a particular model or language is defined and applicable.
            Helps in managing complexity by dividing a large domain into smaller, more manageable parts.
            . Entity:
            An object that is fundamentally defined not by its attributes but by a thread of continuity and identity.
            Examples include Customer, Product, Order, etc.
            . Value Object:
            An object that describes certain aspects of a domain with no conceptual identity.
            Often immutable and can be shared across entities.
            Examples include Address, Money, DateRange, etc.
            . Aggregate:
            A cluster of associated objects that we treat as a unit for data changes.
            An Aggregate has a root entity known as the Aggregate Root, which is responsible for maintaining consistency within the Aggregate.
            . Repository:
            A mechanism for encapsulating storage, retrieval, and query behavior which emulates a collection of objects.
            . Service:
            A stateless operation that is meaningful in the domain but is not a part of any entity or value object.
            Represents domain operations that don't naturally fit into an entity or value object.
            . Domain Events:
            Represents an event that has happened within the domain.
            Used to communicate between different parts of the domain or to trigger side effects.
            - Steps to Apply DDD:
            . Understand the Domain:
            Collaborate closely with domain experts to understand the problem space and domain concepts.
            . Model the Domain:
            Use concepts like Entities, Value Objects, Aggregates, and Bounded Contexts to model the domain.
            . Implement Ubiquitous Language:
            Use the same terminology in the code as used by domain experts to ensure a shared understanding.
            . Focus on Core Domain:
            Identify and focus efforts on the core business domain that adds the most value.
            . Iterative Refinement:
            Continuously refine and evolve the domain model based on new insights and changing requirements.
            - Benefits of DDD:
            . Clearer Communication:
            Establishes a common language between technical and non-technical stakeholders.
            . Maintainable and Evolvable Code:
            Provides a structured approach to domain modeling, leading to code that is easier to understand and modify.
            . Focus on Business Value:
            Aligns development efforts with core business needs, leading to more valuable software solutions.
            . Scalability:
            Helps in managing complexity and scalability by breaking down the domain into manageable parts.
            Domain-Driven Design is particularly useful for complex and evolving domains where understanding the business context is critical to delivering successful software solutions. Applying DDD principles can lead to more resilient and adaptable software architectures.
    MVC-MVP-MVVM:
        . MVC, MVP, and MVVM are three popular architectural patterns used in software development, particularly in the context of building user interfaces (UI) for applications. These patterns help to separate concerns, improve code organization, and enhance maintainability. Let's explore each pattern:
        1. Model-View-Controller (MVC):
            Model: Represents the data and business logic of the application. It encapsulates the application's data and behavior.
            View: Represents the presentation layer of the application. It displays the data to the user and sends user input back to the controller.
            Controller: Acts as an intermediary between the model and the view. It receives input from the user via the view, processes the user's actions, updates the model accordingly, and updates the view with the model's data.
            Key Concepts:
                Separation of Concerns: MVC separates the application into three interconnected components, each with specific responsibilities.
                Reusability: Views can be reused with different controllers, promoting modularity and code reusability.
                Testability: Components are decoupled, allowing for easier unit testing of models, views, and controllers independently.
        2. Model-View-Presenter (MVP):
            Model: Represents the data and business logic of the application, similar to MVC.
            View: Represents the UI layer, responsible for displaying data to the user and capturing user input.
            Presenter: Acts as an intermediary between the model and the view. The presenter retrieves data from the model, formats it, and updates the view. It also handles user inputs from the view, processes them, and interacts with the model accordingly.
            Key Concepts:
                Passive View: The view in MVP is passive and does not have direct access to the model. It relies entirely on the presenter for data updates and interactions.
                Testability: MVP facilitates unit testing by separating the presentation logic (presenter) from the UI components (view).
        3. Model-View-ViewModel (MVVM):
            Model: Represents the data and business logic of the application.
            View: Represents the UI layer, similar to MVC and MVP.
            ViewModel: Acts as an intermediary between the model and the view. It exposes data and commands that the view can bind to. The ViewModel transforms the model data into a format suitable for presentation in the view.
            Key Concepts:
                Data Binding: MVVM leverages data binding techniques to establish a two-way communication between the view and the ViewModel. Changes in the ViewModel automatically update the view, and vice versa.
                Separation of Concerns: MVVM separates the presentation logic (ViewModel) from the UI logic (View), enabling better code organization and maintainability.
                Testability: Like MVP, MVVM promotes testability by isolating the presentation logic from the UI components.
        Comparison:
            MVC emphasizes a clear separation between data (Model), presentation (View), and user interaction (Controller).
            MVP introduces a more passive view and moves all presentation logic to the presenter, enabling better testability.
            MVVM leverages data binding to establish a strong relationship between the view and the ViewModel, reducing boilerplate code and simplifying UI updates.
        . Each architectural pattern has its strengths and is suitable for different project requirements and development scenarios. Choosing the right pattern depends on factors such as project complexity, team expertise, scalability requirements, and preferred development practices.
Dev / Clean Code / Documentation / Commenting -_-_- Backend / DSA / Tools / Technologies / Redis / Docker
.
Microservices
    Microservices vs Monolith is a comparison between two architectural styles used in software development. Each approach has its own advantages and challenges, and the choice between them depends on various factors including project requirements, scalability needs, team capabilities, and deployment environment. Let's compare Microservices and Monolith architectures:
    Monolithic Architecture:
        . Definition:
        Monolithic architecture is a traditional approach where all components of a software application are tightly integrated and deployed as a single unit.
        The entire application (e.g., frontend, backend, database) is developed, deployed, and scaled as a single unit.
        . Characteristics:
        Tightly Coupled: Components are interdependent, making it challenging to modify or scale individual parts without affecting the entire system.
        Single Codebase: All features and functionalities are developed within a single codebase, usually organized as modules or layers.
        Simpler Development: Easier to develop and initially deploy since there's only one application to manage.
        Easier Data Consistency: Transaction management and data consistency are straightforward within a single database.
        . Advantages:
        Simplicity: Easier to develop, test, and deploy compared to distributed systems.
        Simplified Deployment: Deployment is straightforward as the entire application is deployed as a single unit.
        Easier Debugging: Debugging and troubleshooting are simpler due to centralized logging and monitoring.
        . Challenges:
        Scalability: Scaling components independently can be challenging, especially for large-scale applications.
        Limited Technology Stack: Limited flexibility in using different technologies and frameworks for different parts of the application.
        Maintenance: Monolithic applications can become complex and difficult to maintain as they grow in size and complexity.
    Microservices Architecture:
        . Definition:
        Microservices architecture is an approach where an application is divided into smaller, independent services that are developed, deployed, and maintained separately.
        Each service focuses on a specific business capability and communicates with other services through well-defined APIs.
        . Characteristics:
        Loosely Coupled: Services are independent and communicate through lightweight protocols like HTTP or messaging queues.
        Distributed Development: Each service can be developed, deployed, and scaled independently by small teams.
        Polyglot Architecture: Different services can use different programming languages, frameworks, and databases based on specific requirements.
        Scalability: Services can be scaled independently based on demand, improving overall system scalability.
        . Advantages:
        Scalability: Easier to scale individual services based on workload and traffic patterns.
        Flexibility: Allows using the best technology stack for each service, promoting innovation and experimentation.
        Improved Fault Isolation: Failures in one service do not affect the entire application, improving overall resilience.
        Team Autonomy: Small teams can work independently on services, promoting agility and faster development cycles.
        . Challenges:
        Distributed Complexity: Distributed systems introduce complexity in terms of communication, data consistency, and monitoring.
        Service Coordination: Requires robust service discovery, load balancing, and error handling mechanisms.
        Increased Operational Overhead: Managing multiple services introduces operational challenges such as deployment automation, monitoring, and logging.
    Choosing Between Microservices and Monolith:
        Project Size and Complexity: For small to medium-sized projects with simpler requirements, a monolithic architecture may be sufficient.
        Scalability Requirements: Applications expecting rapid growth or requiring dynamic scalability often benefit from microservices architecture.
        Team Structure and Capabilities: Microservices are suitable for larger teams or organizations where teams can manage and operate independently.
        Deployment Environment: Cloud-native environments and container orchestration platforms (like Kubernetes) provide excellent support for microservices.
        In summary, microservices architecture offers scalability, flexibility, and resilience but introduces complexity and operational challenges. Monolithic architecture is simpler and easier to manage but can become difficult to scale and maintain as applications grow. The choice between the two depends on specific project requirements, team capabilities, and long-term scalability goals.
Agile-Scrum-Sprint-Daily
    . In the context of software development, Agile is an iterative approach to project management and software development that promotes flexibility, collaboration, and customer satisfaction. Scrum is one of the most popular frameworks used within Agile methodologies. Within Scrum, a Sprint is a time-boxed iteration of work, typically lasting 1-4 weeks, during which a cross-functional team works to complete a set of prioritized tasks from the product backlog. The Daily Scrum (or Daily Standup) is a short, daily meeting held by the Scrum team to synchronize activities and make any necessary adjustments to achieve the Sprint goal. Let's delve into each of these concepts in more detail:
    Agile:
        Definition: Agile is a set of principles and values outlined in the Agile Manifesto, emphasizing iterative development, customer collaboration, responding to change, and delivering working software frequently.
        Key Principles:
            Customer collaboration over contract negotiation.
            Responding to change over following a plan.
            Individuals and interactions over processes and tools.
            Working software over comprehensive documentation.
        Benefits:
            Enhanced flexibility and adaptability to changing requirements.
            Improved customer satisfaction through frequent deliveries and collaboration.
            Emphasis on continuous improvement and responsiveness.
    Scrum:
        Definition: Scrum is a lightweight Agile framework for iterative development, primarily used for software development projects. It emphasizes cross-functional teams, iterative progress, and continuous improvement.
        Roles:
            Product Owner: Represents the stakeholders and defines the product vision and priorities.
            Scrum Master: Facilitates the Scrum process, removes impediments, and ensures adherence to Scrum practices.
            Development Team: Cross-functional team responsible for delivering the product increment.
        Artifacts:
            Product Backlog: Prioritized list of features, enhancements, and fixes for the product.
            Sprint Backlog: Subset of items from the product backlog selected for implementation during the Sprint.
            Increment: Potentially shippable product increment created during each Sprint.
        Events:
            Sprint Planning: Meeting where the Scrum team plans the work to be done during the Sprint.
            Daily Scrum: Short, daily meeting to synchronize activities and discuss progress and impediments.
            Sprint Review: Meeting at the end of the Sprint to demonstrate the completed work and gather feedback.
            Sprint Retrospective: Meeting to reflect on the Sprint and identify improvements for the next Sprint.
    Sprint:
        Definition: A Sprint is a time-boxed iteration of work, typically lasting 1-4 weeks, during which a Scrum team works to deliver a potentially shippable product increment.
        Characteristics:
            Sprints have fixed durations and aim to produce a tangible outcome (product increment).
            Sprints enable frequent inspection and adaptation of the product and process.
            Each Sprint begins with Sprint Planning and ends with a Sprint Review and Sprint Retrospective.
    Daily Scrum (Daily Standup):
        Definition: The Daily Scrum is a short, time-boxed meeting held by the Scrum team (including the Product Owner and Scrum Master) to synchronize activities and discuss progress toward the Sprint goal.
        Purpose:
            Share updates on what each team member has accomplished since the last meeting.
            Discuss plans for the day and identify any potential obstacles or impediments.
            Align efforts toward achieving the Sprint goal and address any issues that may impact progress.
        Format:
            Participants stand to encourage brevity (hence the term "Daily Standup").
            Each team member answers three questions: What did I accomplish yesterday? What will I do today? Are there any impediments blocking my progress?
    . Benefits of Scrum and Daily Scrum:
        Improved Communication: Daily Scrum promotes transparency and communication among team members.
        Enhanced Collaboration: Scrum fosters cross-functional collaboration and accountability.
        Frequent Inspection and Adaptation: Sprints and Daily Scrums enable teams to inspect progress regularly and adapt to changes effectively.
    . Overall, Agile methodologies like Scrum and practices such as Sprints and Daily Scrums are widely adopted in the software industry for their ability to deliver value iteratively, respond to change, and foster collaboration among teams. Each element plays a crucial role in ensuring project success and continuous improvement.
.
Limit-Derivative-Integral / Statistics&Probability / LinearAlgebra
.
PreProcessing / ML Models / ML Modeling / After(pycaret , streamlit)
    DS , ML
        Concepts, Math, DSA
        Code, Dev, Backend, SE, Tools, Technologies
        numpy,pandas , matplotlib,seaborn , sklearn,pycaret
        -
        Supervised Learning
            Supervised learning tasks can be categorized into two main types: Regression, Classification
            Key algorithms used in supervised learning include linear regression, logistic regression, decision trees, random forests, support vector machines (SVM), naive Bayes, and neural networks (including deep learning models).
            The performance of a supervised learning model is typically evaluated using metrics such as mean squared error (MSE) for regression tasks, accuracy, precision, recall, F1-score for classification tasks, and others depending on the specific problem domain and goals.
        UnSupervised Learning
            Unsupervised learning tasks can be broadly categorized into two main types: Clustering, Dimensionality Reduction
            Key characteristics of unsupervised learning include:
                Exploratory Analysis: Unsupervised learning is often used for exploratory analysis of data to gain insights and understanding without preconceived notions or labeled examples.
                Feature Extraction: Unsupervised learning can be used to extract meaningful features or representations from raw data, which can then be used for downstream tasks.
                Anomaly Detection: Unsupervised learning can also be applied to detect anomalies or outliers in data by identifying instances that deviate significantly from normal patterns.
        Reinforcement Learning
        Regression
        Classification
        Feature Scaling
        Missing Values
            Deletion, Imputation
        SVM (Support Vector Machine)
            Classification and Regression
        Confusion Matrix
            Performance measure for Classification - TP TN FP FN
        Linear Regression
        Logistic Regression
            Two Classes
        Underfitting / Overfitting
            Underfitting
                can try using a more complex model
                adding more features
                tuning hyperparameters to improve performance
            Overfitting
                cross-validation
                regularization
                    L1/L2 regularization
        Dimension Reduction
            PCA (Principal Component Analyse)
            t-SNE (t-distributed Stochastic Neighbor Embedding)
        KNN (K Nearest Neighbors)
            Classification and Regression
        K Fold
            K Fold Cross Validation
            for validation and evaluation
        K-Means
            Clustering
        K Neighbors Classifier
        K Neighbors Regressor
        Scaling
            Min-Max Scaling Normalization
            Standardization, z-score standardization
        Feature Extraction
            Creating feature vectors
            PCA (Principal Component Analyse)
            TF-IDF (term frequency-inverse document frequency)
            Edge detection
        Hyper Parameters
        Feature Selection
            Filter Methods
                Correlation
                Mutual Information
            Rapper Methods
                Forward Selection
                Backward Elimination
                Recursive Feature Elimination (RFE)
            Embeded Methods
                Lasso (L1 Regularization)
                Decision tree-based feature importance
        AutoML
        Transform, fit_transform
        XGB
            XGBoost
            XGB Regressor
            XGB Classifier
        Bagging (Bootstrap Aggregating)
            is an Ensemble
            Random Forest is a popular algorithm that uses bagging with decision trees as base models.
            bagging is a powerful ensemble learning technique that leverages the idea of training multiple models independently and combining their predictions to achieve better performance and generalization in machine learning tasks.
        Gradient Boosting
            Gradient boosting algorithms like XGBoost (eXtreme Gradient Boosting) and LightGBM (Light Gradient Boosting Machine) have gained popularity due to their effectiveness in producing accurate predictions with strong generalization capabilities.
        Ensemble Learning
            Ensemble learning is a machine learning technique that involves combining multiple individual models (called base learners or weak learners) to produce a more accurate and robust predictive model. The idea behind ensemble learning is to leverage the diversity and collective wisdom of multiple models to improve overall prediction performance.
            Bagging (Bootstrap Aggregating)
            Random Forest
            Boosting
            Gradient Boosting Machines (GBM)
            AdaBoost (Adaptive Boosting)
        DBSCAN
            Clustering
            DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular clustering algorithm used in data mining and machine learning to identify clusters of data points based on their density in a feature space. Unlike traditional clustering algorithms (e.g., K-means) that require specifying the number of clusters beforehand, DBSCAN is capable of discovering clusters of arbitrary shapes and sizes without prior assumptions about the number of clusters.
        SVD (Singular Value Decomposition)
            SVD, or Singular Value Decomposition, is a fundamental matrix factorization technique used in linear algebra and numerical computing. It is widely applied in various fields, including signal processing, image compression, natural language processing, and machine learning.
            Applications:
                Dimensionality Reduction
                Low-Rank Approximation
                Matrix Reconstruction
                Feature Engineering
                Collaborative Filtering
        Erro type 1 and 2
            Type I Error (False Positive)
            Type II Error (False Negative)
            Type I Error: Incorrectly rejecting a true null hypothesis (false positive).
            Type II Error: Failing to reject a false null hypothesis (false negative).
        Null Hypothesis / H0
            The Null Hypothesis (denoted as H0) is a fundamental concept in statistical hypothesis testing that represents the default assumption or status quo about a population parameter or relationship between variables. It is used to assess whether there is sufficient evidence in the data to reject this assumption in favor of an alternative hypothesis.
        Alternative Hypothesis / H1
            The alternative hypothesis (H1) represents the complement to the Null Hypothesis and is typically what researchers aim to support if the Null Hypothesis is rejected.
            The Alternative Hypothesis (denoted as H1) is a counterpart to the Null Hypothesis in statistical hypothesis testing. It represents an alternative explanation, effect, or relationship that is considered if there is sufficient evidence to reject the Null Hypothesis based on the observed data.
        H0 and H1
            H0 and H1 are commonly used symbols in statistical hypothesis testing to represent the Null Hypothesis and the Alternative Hypothesis, respectively.
        Gradient Descent
        Stochastic Gradient Descent
        Cost Function
        Line Search
        Batch Gradient Descent
        Mini Batch Gradient Descent
        Bagging Classification
            is a Ensemble Learning
        Random Forest Classification
            An Ensebmble Learning which uses Decision Trees
        Standard Scaler
        Cross Validate
            Folds
            K-Fold Validation
        Poission Regressor
            Counts of events
        Naive Bayes (Naive Bayes in DS and ML)
        Naive Bayes Classification
        Bernoulli Naive Bayes Classifier
        Gaussian Naive Bayes Classifier
        Accuracy Score
        r2 score
        Mean Absolute Error
        Mean Squared Error
        F1-score
        Precision and Recall
        Evaluation
        Encoding Categorical Data
        Column Transformer
        OneHotEncoding
        Label Encoder
        MinMaxScaler
        cross val score
        Grid Search CV(Cross Validation)
        SVC
        SVR
        Decision Tree
        Decision Tree Classifier
        Decision Tree Regressor
        Random Forest
            is an Ensemble Learning uses multiple Decision Trees.
        Random Forest Classifier
        Random Forest Regressor
        Ensemble Learning
            Bagging
                Random Forest
            Boosting
                Ada Boosting
            Stacking
        -
        LDA (Linear Discriminant Analysis)
            Dimension Reduction
        SFS (Sequential Forward Selection)
            Feature selection
        SBS (Sequential Backward Selection)
            Feature selection
        MI (Mutual Information)
            dependency between variables.
        OLS (Ordinary Least Square) Solution Stability
        Robust Scaler
            scaling using robust technique in statistics.
.
Supervised Learning:
    Learning from labeled data where the algorithm learns to map input to output based on example input-output pairs.
Unsupervised Learning:
    Learning from unlabeled data to find patterns or structures in the data.
Reinforcement Learning:
    Learning through trial and error by interacting with an environment to maximize cumulative reward.
Regression:
    Predicting continuous outcomes based on input features.
Classification:
    Predicting categorical outcomes based on input features.
Feature Scaling:
    Normalizing or standardizing input features to ensure they have similar scales.
Missing Values:
    Handling data points that are not available or are incomplete.
Random Forest:
    An ensemble learning method that constructs multiple decision trees and outputs the mode of the classes or the mean prediction of the individual trees.
XGBoost:
    An optimized gradient boosting algorithm used for supervised learning tasks.
SVM (Support Vector Machine):
    A supervised learning model used for classification and regression analysis.
Linear Regression:
    A basic regression technique to model the relationship between dependent and independent variables.
Confusion Matrix:
    A table used to evaluate the performance of a classification model.
Bagging:
    Bootstrap aggregating, a technique used to improve the stability and accuracy of machine learning algorithms.
Gradient Boosting:
    A machine learning technique for regression and classification problems.
Underfitting / Overfitting:
    Underfitting refers to a model that is too simple to capture the underlying pattern of the data, while overfitting refers to a model that is overly complex and learns noise as well.
Ensemble Learning:
    Combining multiple models to improve prediction accuracy.
PCA (Principal Component Analysis):
    A technique for dimensionality reduction.
Dimension Reduction:
    Reducing the number of random variables to analyze data more effectively.
KNN (K-Nearest Neighbors):
    A simple and effective classification algorithm.
K-Means:
    An unsupervised learning algorithm used for clustering.
DBSCAN:
    Density-based spatial clustering of applications with noise.
SVD (Singular Value Decomposition):
    A method used for matrix factorization.
Scaling:
    Adjusting the range of features to be on a similar scale.
LDA (Linear Discriminant Analysis):
    A technique used for feature extraction and dimensionality reduction.
Feature Extraction:
    Deriving meaningful information from raw data to improve performance.
Hyper Parameters:
    Parameters that are set before the learning process begins.
Feature Selection:
    Selecting a subset of relevant features to build models.
SFS (Sequential Forward Selection) and SBS (Sequential Backward Selection):
    Techniques for feature selection by adding or removing features.
Mutual Information (MI):
    A measure of the amount of information obtained about one variable through the other variable.
AutoML:
    Automated machine learning, where the process of building machine learning models is automated.
Error Type 1 and 2:
    Type I error (false positive) and Type II error (false negative) in hypothesis testing.
Null Hypothesis and Alternative Hypothesis:
    Statements about the population parameters in statistics.
OLS (Ordinary Least Squares) Solution Stability:
    Stability analysis of solutions derived from linear regression.
Gradient Descent:
    An optimization algorithm used to minimize the loss function.
Stochastic Gradient Descent:
    A variant of gradient descent where a random sample is used to compute the gradient.
Cost Function:
    A function that measures the performance of a machine learning model.
Line Search:
    A method used to find the local minimum of a function.
Mini Batch Gradient Descent:
    A variant of stochastic gradient descent that processes small batches of data.
Logistic Regression:
    A classification algorithm used to model the probability of a certain class or event.
Cross Validation:
    A technique used to assess the performance of a model.
Poisson Regressor:
    A regression model for count data.
Bernoulli Naive Bayes Classifier and Gaussian Naive Bayes Classifier:
    Probabilistic classifiers based on Bayes' theorem.
Accuracy Score, r2 Score, Mean Absolute Error (MAE), Mean Squared Error (MSE), F1-Score:
    Metrics used to evaluate the performance of models.
Evaluation:
    Assessing the performance of machine learning models.
Encoding Categorical Data:
    Transforming categorical variables into numerical representations.
Column Transformer:
    A scikit-learn utility for applying different transformers to different columns of a pandas DataFrame.
OneHotEncoding and Label Encoder:
    Techniques for encoding categorical variables.
Robust Scaler and MinMaxScaler:
    Methods for scaling features.
Cross Val Score:
    A function in scikit-learn used for cross-validation.
Grid Search CV:
    A technique for tuning hyperparameters.
K Neighbors Classifier, SVC (Support Vector Classifier), Decision Tree Classifier, Decision Tree Regressor, Random Forest Regressor, XGB Classifier:
    Different machine learning models used for classification or regression tasks.
K Fold:
    A type of cross-validation technique where the dataset is divided into k subsets for training and testing.

.
تعریفشون رو بلد باشی، ابزار مناسب برای اونها چیه و چطور کار میکنه، کد و پیاده سازی شون رو بلد باشی و انجام بدی

تعریف، ابزار مربوطه، کد
.

AI/DS/ML/DL
Supervised Learning
UnSupervised Learning
Reinforcement Learning
Regression
Classification
AutoML
-
Line Search (used in optimization algorithms to find an optimal step size to a way)
Cost Function (the loss function, measures the difference between actual values and predicted values)
Missing Values (how to handle them?)
Confusion Matrix (evaluates the accuracy of a classification model)
Hyper Parameters (parameters that are set before the learning/training begins)
Dimension Reduction (reduce the number of features or dimentions to simplifying without sacrificeing much predictive power)
Regularization / L1/L2 (to prevent overfitting, adding a penalty to prevent this; L1 and L2 prevent overfitting; L1 regularization (Lasso) adds a penalty to the loss function and this encourages sparsity; L2 regularization (Ridge) adds a penalty to the loss function and it encourages sparsity but not as aggresvie as the L1 regularization)
Scaling / Feature Scaling (adjust the range of features to a similar scale like between 0 and 1 or a range with mean of 0 and std of 1; we can imporve the performance and stability of ml models and preventing any single feature from dominating the learning rate)
Underfitting / Overfitting (Underfitting is when a model is to simple and performs poorly not onlty on training data but also on new and unseen data and it happens when the model is too simple and basic relative to complexity of the data to capture the underlting patters of the data; Overfitting happens when the model learns the dateils and noise in the data and it negatively impacts the model's performance on new and unseen data and overfitting occurs when the models is to complex relative to the amount and noise level in the traing data; to address underfitting we can use more complex models or increase models complexity by adding more features; to address overfitting we can use techniques like regularization or cross-validation or reducing model cimplexity)
MinMaxScaler / Standard Scaler (MinMaxScaler transforms features by scaling them to a specified range typically between 0 and 1; StandardScaler standardizes features by removing the mean and scaling to unit variance which makes the data more Gaussian-like and usually it is a range with mean of 0 and std of 1)
SVD (Singular Value Decomposition) (is a fundamental matrix decomposition; it decomposes a matrix into three simpler matrices and enabling efficient computation and analysis of the original matrix's properties; applications are: dimensionality reduction, matrix approximation, matrix inversion)
Feature Extraction / Feature Selection (FeatureExtraction involves transforming raw data into a set of features that are more informative and suitable for ml algorithms; FeatureSelection involves selectinga subset of the most relevant features from the original set of features and the goal is to reduce the dimentionality of the data while retaining the most important information for modeling and it helps improve model performance, reduce overfitting and enhance interpretability; in summary, feature extraction transforms data into meaningful features, while feature selection chooses the most important features to use in ml models)
Mean Absolute Error / Mean Squared Error (Are common metrics used to evaluate the performance of regression models; MAE is a metric which measures the average magnitude of errors between predicted and actual values and it is calculated as the mean of the absolute differences between predicted and actual values; MSE is a metric which measures the average of the squared differences between predicted and actual values and it penalizes large errors more than smaller errors and is more sensitive to outliers compared to MAE)
Transform, fit_transform / Column Transformer (ColumnTransformer allows for applying different preprocessing steps to different columns while transform and fit_transform are methods used to apply transformations to data based learnd parameters using fit method)
OneHotEncoding / Label Encoder / Encoding Categorical Data (OneHotEncoding is a technique used to convert categorical variables into a binary format that can be used by ml algorithms an each category is represented as a binary vector where only one bit hot (1) indicating the presence of that category; LableEncoder is a technique used to convert categorical labels into numerical representations and each unque label is mapped to an integer value and is useful when dealing with ordinal categorical variables where the order of labels matters; EncodingCategoricalData is refers to the process of converting categorical data into numerical format)
Null Hypothesis / Alternative Hypothesis / H0 and H1 / Erro type 1 and 2 (Type 1 error also known as a false positive and it occurs when you reject a true null hypothesis or H0 and it's the probability of incorrectly concluding that there is an effect difference when there actually isn't; Type 2 error also known as a false negative and it occurs when you fail to reject a false null hypothesis or H0 and it's the probability of incorrectly concluding that there is no effect or difference when there actually is; NullHypothesis/H0 is a statement that suggest there is no effect or relasionship in the population being studied and it's typically the hypothesis to be tested against an alternative hypothesis; AlternativeHypothesis/H1/Ha is a statement that contradicts the null hypothesis and suggests there is an effect or relationship in the population and it's what researchers are trying to find evidence for)
Gradient Descent / Batch Gradient Descent / Mini Batch Gradient Descent / Stochastic Gradient Descent (GradientDescent is an optimization algorithm used to minimize a function by iteratively moving the direction of the steepest descent of the function's gradient; BatchGradientDescent is a variant of gradient descent that computes the gradient using the entire dataset and it updates the model parameters after evaluating all training examples, making it slower but more accurate for convergence; MiniBatchGradientDescent is a variant of gradient descent that computes the gradient using a subset or mini-batch of the dataset and it strikes a balance between batch gradient descent and stochastic gradient descent by updating the model parameters based on batches of data which improves training efficiency; StochasticGradientDescent/SGD is a variant of gradient descent that computes the gradient using a single training example at a time and it updates the model parameters frequently and making it faster but more susceptible to noise and fluctuations)
Accuracy Score / r2-score / F1-score / cross val score / Precision & Recall / Cross Validate / Evaluation / K Fold (AccuracyScore measures the proportion of correct predictions both true positive and true negative, made by a classifier; r2-score(Coefficient of Dtermination) measures the proportion of the variance in the dependent variable that is predictable from the independent variables and r2-score ranges from 0 to 1 where 1 indicates a perfect fit; F1-score is the harmonic mean of precision and recall, providing a balance between these two metrics and it's useful for binary classification tasks where precision is the ratio of true true positive predictions to all positive predictions and recall is the ratio of true positive predictions to all actuall positives; CrossValidScore/CrossValidationScore it evaluates a model's performance by splitting the data into multiple subsets (folds), training the model on several combinations of these subsets and then averaging the results which helps assess a model's generalization performance; K-Fold-Cross-Validation is a specific type of cross-validation where the data is divided into k subsets (folds) and the model is trained and evaluated k times and each time using a different fold for evaluating and the remaining folds for training which helps to provide a more reliable estimate of a model's performance compared to a single train-test split; Precision, is the proportion of true positive predictions (correctly predicted positive) out of all positive predictions made by a classifier and it measures the accuracy of positive predictions; Recall, also known as sensitivity or true positive rate, is the proportion of true positive predictions (correctly predicted positive) out of all actual positive instances in the data and it measures the ability of a classifier to identify all relevant instances (true positive))
-
Linear Regression (Is a statistical method used to model the relation between a dependent variable and one or more indeoendent variables and the goal is to find a linear equation that best predicts the value of Y based on the values of X and the coefficients are estimated from the data to minimize the difference between predicted and actual Y values)
Logistic Regression (Is a statistical method used for binary classification tasks where the dependent variable Y is categorical with two possible outcomes and it estimates the probability that Y belongs to a particular category based on one or more independent variables, and instead of predicting the actual outcome directly, logistic regression models the log-odds of the probability of Y being in a specific category and the output of logistic regression is a probability score between 0 and 1)
SVM (Support Vector Machine) / SVC / SVR (SVM is a supervised machine learning algorithm used for both classification and regression tasks and it works by finding the hyperplane that best separates different classes in a high-dimensional space, it aims to maximize the margin between the classes which helps in achieving good generalization to new data; SVC is specially the svm algorithm used for classification tasls and it finds the optimal hyperplane that best divides the classes in the feature space and is effective for binary and multiclass classification problems; SVR is the svm algorithm used for regression tasks and instead of predicting discrete class labels, it works by finding a hyperplane that maximizes the error between predicted and actual values while still maximizing the margin)
KNN (K Nearest Neighbors) / K-Means / K Neighbors Classification / K Neighbors Regression (KNN used for classification and regression and works based on the principle of similarity where new data points are classified based on the majority class or average value of their nearest K neaighbors and requires choosing a value for K which represents the number of neighbors to consider; K-Means is an unsupervised algorithm used for clustering and divides adataset into K clusters based on similarity with each cluster represented by its centroids (center points) and works by iteratively assigning data points to the nearest cluster centroid and updating the cebtroids based on the mean of the assigen points)
Naive Bayes (Naive Bayes in DS and ML) / Naive Bayes Classification / Bernoulli Naive Bayes Classification / Gaussian Naive Bayes Classification (NaiveBayes assumes that the features are conditionally independent given the class label which simplifies the computation, and it often performs well and efficient for text classification; NaiveBayesClassification refers to using the naive bayes algorithm for classification and it calculates th eprobability of a data point belonging to each class based on the observed features and then selects the class with the highest probability as the prediction; BernouliNaiveBayesClassification a specific variant of naive bayes suited for binary feature data presence or absence of a feature, it assumes that features are binary and uses the bernouli distribution to model the liklihood of each feature given the class; GaussianNaiveBayesClassification a variant of naive bayes suitable for continuous feature data assumed to follow a gaussian/normal distribution, it assumes that features are normally distributed within each each class and estimates mean and variance and variance parameters for each feature and class)
- = ---------------------------------------------------------------------------
Bagging (Bootstrap Aggregating) / Bagging Classification / Bagging Regression (Bagging/BotstrapAggregation involves creating multiple models using subsets of the training data and it uses a technique called bootstraping where random samples of data are repeatedly draw with replacement and the final prediction is often an average for regression or a majority vote for classification of predictions from these models and the goal is to reduce variance and improve model performance by reducing overfitting; BaggingClassification multiple classifiers like decision trees are trained on different subsets of the training data and each classifier gives a prediction and the final prediction is determind by majority voting among these classifiers; BaggingRegression multiple regression models are trained on different bootstrapped samples of the training data and the final regression output is often the average oft he outputs from these models)
Gradient Boosting / XGB/XGBoost (GradientBoosting is a ml technique used for builing predictive models particularly decision trees and it builds models in a sequential manner where each new model corrects errors made by the previous model and the key idea is to optimize a loss function by minimizing errors through an iterative process; XGB/XGBoost/ExtremeGradientBoosting is a popular implementation of gradient boosting designed to be highly efficient and scalable and it includes several enhancements over traditional gradient boosting such as regularization and parallel processing)
Ensemble Learning (is a ml technique where multiple models are combined to improve the overall performance and accuracy of the predictive model instead of relying on a single model and common ensemble methods include bagging(like random forest) and boosting (like gradient boosting) and stacking)
DBSCAN (stands for Density Based Spatial Clustering of Applications with Noise, it's a popular clustering algorithm used in machine learning and data mining, and is designed to identify clusters of points based on the density of data points in the feature space and it doesn't requires specifying the number of clusters beforehand and making it useful for datasets where the number of clusters is not know and the algorithm groups together points that are closely packed and defining clusters as regions of low density (noise) and it handle noise points and outliers effectively as they are not assigned to any specific cluster; in summery it is a density based clustering algorithm that automatically identifies clusters in data based on density and is robust to outliers and mnoise in the dataset)
Poission Regression (is a type of regression analysis used when the dependent variable is a count or an event occurrence whithin a fixed period of time or area and it models the relationship between the dependent variable (count data) and one or more independent variables assuming a poisson distribution for the dependent variable, and isused to model count data assuming a poisson distribution for the dependent variable and it's especially useful when dealing with data that represent counts of events)
Grid Search CV/CrossValidation (is a technique used in ml for hyperparameter tuning which is the process of finding the optimal hyperparameters for a model; it exhaustively searches through a specific parameter grid to find the best combination of hyperparameters for a given model; it evaluates each combination using cross-validation to assess the model's performance and prevent overfitting; helps automate the process of tuning hyperparameters and saving time and effort compared to manual tuning; this technique is commonly used to optimize parameters for models like support vector machines, decision trees, random forest and more)
Decision Tree / Decision Tree Classification / Decision Tree Regression (DecisionTree is a tree-like model used for making decisions based on features or attributes of data and it is a supervised learning algorithm that partitions the data into subsets based on featur vakues, creating a tree structure of decisions and each internal node represents a feature and each branch represents a decision rule based on that feature and each leaf node represents the outcome or class label; DecisionTreeClassification is a type of supervised learning where the goal is to predict a categorical target variable (class label) the decision tree algorithm learns rules from the training data to classify instances into different predefined classes or categories and it's efficient for both binary and multi-class classification tasks and is interpretable due to its tree-like structure; DecisionTreeRegression is a supervised learning technique used for predicting continiuous target variables (numeric values) instead of predicting class labels, predicts the value of the target variable based on the features)
Random Forest / Random Forest Classification / Random Forest Regression (RandomForest is an ensemble learning technique based ondecision trees and it creates multiple decision trees during training and combines their predictions through voting for classification or averaging for regression and the randomness comes from using bootstrap samples of the data and rendom subsets of features for training each tree; RandomForestClassification uses the random forest technique for predicting categorical outcomes (class labels) and it trains multiple decision trees on random subsets of the training data and combines their predictions through majority voting and is less prone to overfitting compared to individual decision trees; RandomForestRegression applies the random forest technique to predict continuous numerical values and it builds an ensemble of decision trees that collectievly predict the target variable and the final prediction is the average of predictions from all trees)
- = -
LDA (Linear Discriminant Analysis)
SFS (Sequential Forward Selection)
SBS (Sequential Backward Selection)
MI (Mutual Information)
OLS Solution Stability
Robust Scaler
.
