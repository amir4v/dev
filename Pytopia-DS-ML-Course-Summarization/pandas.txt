NumPy = Numerical Python
Pandas = Panel Data

Cross sectional data
    means that we have data from many units, at one point in time.
Time series data
    means that we have data from one unit, over many points in time.
Panel data (or time series cross section)
    means that we have data from many units, over many points in time.

Data Lake
    A data lake is a large storage repository that holds a huge amount of
    raw data in its original format until you need it. Data lakes exploit
    the biggest limitation of data warehouses: their ability to be more flexible.
    As we’ll see below, the use cases for data lakes are generally limited to
    data science research and testing—so the primary users of data lakes are
    data scientists and engineers. For a company that actually builds data
    warehouses, for instance, the data lake is a place to dump and temporarily
    store all the data until the data warehouse is up and running.
    Small and medium sized organizations likely have little to no reason to use a data lake.
Data Warehouse
    The next step up from a database is a data warehouse.
    Data warehouses are large storage locations for data that you accumulate
    from a wide range of sources. For decades, the foundation for
    business intelligence and data discovery/storage rested on data warehouses.
    Their specific, static structures dictate what data analysis you could perform.
    Data warehouses are popular with mid- and large-size businesses as a way of
    sharing data and content across the team- or department-siloed databases.
    Data warehouses help organizations become more efficient. Organizations
    that use data warehouses often do so to guide management decisions—all
    those “data-driven” decisions you always hear about.

Series - 1D
DataFrame - 2D
Panel - 3D

The axis labels are collectively referred to as the index.

s = pd.Series(data, index=index)
data can be:
    A Python dict
    A NumPy ndarray
    A scalar value
np.random.randn(5)
    Gives 5 normal distributed numbers between -1 to 1.
pd.Series(np.random.randn(5), index=['a','b','c','d','e'])
s.index
pd.Series({'a':1,'c':2,'b':3})

NaN (Not a Number) is the standard msssing data marker used in pandas.

pd.Series(5, index=['a', 'g'])
    a  5
    g  5

s[0] 0 is the key not index.
s['g']

s[:3]
    This is slicing.

s.median()

s[s > s.median()]

s[ [0,1,2] ]
    0,1,2 are the keys not indices.

np.exp(s)
    e to the power of each element.

s.dtype

s.array
    returns a numpy extension array.
ExtensionArray is a thin wrapper around one or more concrete arrays like a numpy.ndarray.
s.to_numpy()
    returns a numpy ndarray.

Series is dict-like.
'key' in s
s['k'] = 'v'
s.get('k', np.nan)

A key difference between Series and ndarray is that operations between Series
automatically align the data based on label. Thus, you can write computations
without giving consideration to whether the Series involved have the same labels.
The result of an operation between unaligned Series willhave the union of the indexes involved,
if a label is not found the result will be marked as Nan.
For missing value you can use dropna function.
s + s
s * 2
np.exp(s)

Series can also have a name attribute.
pd.Series(np.random.randn(5), name='name-something)
s.nanme
s.name = 'new name'
s.rename('newer name').name

DataFrame
    is a 2-dimensional labeled data structure with columns of potentially different types.
    You can think of it like a spreadsheet or SQL table, or a dict of Series objects.
    It is generally the most commonly used pandas object.

Series
    is a one-dimensional labeled array capable of holding any data type.
    The axis labels are collectively referred to as the index.

You can create a dataframe using:
    Dict of 1D ndarrays, lists, dicts, or Series
    2-D numpy.ndarray
    Structured or record ndarray
    A Series
    Another DataFrame

Along with the data, you can optionally pass index (row labels) and columns (column labels) arguments.
The resulting index will be the union of the indexes of the various Series.

d = {
    'one': pd.Series({'a':2,'d':4}),
    'two': pd.Series({'a':32,'c':45}),
}
df = pd.DataFrame(d)
df = pd.DataFrame(d, index=['a','s','g'])
df = pd.DataFrame(d, columns=['one', 'tt'])
df.index
df.columns
df = pd.DataFrame({'one':np.array([1,2,3]), 'two':[4,5,6]})
    All arrays must be of the same length.
data = np.zeros((2,), dtype=[('A', 'i4'), ('B', 'a10')])
data[:] = [(2, 'Ali'), (45, 'Amir')]
pd.DataFrame(data)
data = [{'a':1, 'b':6}, {'a':4, 'c':5}]
pd.DataFrame(data)

MultiIndexed
pd.DataFrame(
    {
        ("a", "b"): {("A", "B"): 1, ("A", "C"): 2},
        ("a", "a"): {("A", "C"): 3, ("A", "B"): 4},
        ("a", "c"): {("A", "B"): 5, ("A", "C"): 6},
        ("b", "a"): {("A", "C"): 7, ("A", "B"): 8},
        ("b", "b"): {("A", "D"): 9, ("A", "B"): 9},
    }
)
pd.DataFrame(pd.Series([1,2,3]))

from collections import namedtuple
Point = namedtuple('Point', 'x, y')
pd.DataFrame([Point(1,2), Point(3,4)])

from dataclasses import make_dataclass
Point = make_dataclass('Point', [('x', int), ('y', int)])
pd.DataFrame([Point(1,2), Point(3,4)])

pd.DataFrame.from_dict
    takes a dict of dict or array-like sequesnces.
pd.DataFrame.from_dict(dict([ ('A', [1,2,3]) , ('B', [4,5,6]) ]))

pd.DataFrame.from_dict(dict([ ('A', [1,2,3]) , ('B', [4,5,6]) ]),
    orient='index',
    columns=['a','b','c']
)
    it's like .Inverse function in numpy which the A,B are indexes instead of columns.

DataFrame.from_records
    takes a list of tuples or an ndarray with structured dtype.
pd.DataFrame.from_records(
    [
        (1,2,3),
        (4,5,6)
    ]
)
pd.DataFrame.from_records(
    [
    np.ndarray([1,2],dtype='i4'),
    np.ndarray([1,2],dtype='i4')
    ]
)

You can treat a DataFrame semantically like a dict.
df['a'] = df['b'] * df['c']
df['new'] = df['b'] < 3
del df['col']
df.pop('col')
df['col'] = 'val'
    all cells get filled with a scallar static value.
df['col'][:2]
df.insert(1, 'name', df['aCol'])

pd.read_csv('...')

df.info()
df.head()
df.assign(namesth=df['one'])
    and returns the new df

We can also pass in a function of one argument to be evaluated on the DataFrame being assigned to.
df.assign(namecol=lambda x: x['one'] > 1)

%matplotlib inline
df.plot()

%matplotlib inline
df.query('one > 2').assign(
    a=lambda x: x['one'],
    b=lambda x: x['one'],
).plot(
    kind='scatter', x='a', y='b'
)

df[col]: Select column -> Series
df.loc[label]: Select row by label -> Series
df.iloc[loc]: Select row by integer location -> Series
df[5:10]: Slice rows -> DataFrame
df[bool_vec]: Select rows by boolean vector -> DataFrame

df1 + df2
df2 * df1
df - df.iloc[0]
    subtract the first row from all rows.
df + 9
    all cells
1 / df
df1 & df2
df1 | df2
df1 ^ df2
-df

pd.DataFrame([1,0,1], dtype=bool)

Transpose / tara nahade / like inverse
    df.T

np.log
np.exp
np.sqrt
np.remainder(df1, df2)
    df1 % df2

np.asarray(df)

numpy-universal-functions(Pandas Series / Pandas DataFrame)

pd.options.display.max_colwidth = 10
pd.set_option('display.max_colwidth', 10)
pd.options.display.max_rows
pd.options.display.max_columns
pd.options.display.precision
The precision option sets the number of decimals to be display in a dataframe.

df.column_name

.apply
df.apply(lambda x: x)
df['one'].apply(lambda x: x)
df['foo'].apply(lambda x: x.split()[0])

df['foo'].get(0)

pd.date_range('2000/1/1', periods=10)
    day by day
    8 days

df.head(n=5)
df.tail()
df.sample(n=1)
df.sample(15)

df.columns = [...]
df.index = [...]

df.sub(df.iloc[0], axis=1)
df.sub(df['A'], axis=0)
df.sub(df.iloc[0], axis=0)
df.sub(df.iloc[0], axis='rows')
df.sub(df.iloc[0], axis='index')
df.sub(df.iloc[0], axis=1)
df.sub(df.iloc[0], axis='columns')

df.loc[0, 'A']
df.loc['name', 'A']

df.add(df2, fill_value=0)
df.add(df2, fill_value=0).fillna(0)
df.add(df2).fillna()
df.fillna(0)
df.mean()

    Boolean result
.any(axis=0)
.empty()
.all()
.bool()
(df > 0).all().any()
df.any().any()

df.empty == True/False
pd.DataFrame().empty == True

    Error
if df:
    ...
df and df2

np.nan is not equal to np.nan
    (np.nan == np.nan) == False

(df + df) == (df * 2)
    is not necessary True
Series and DataFrames have an .equals(...) method to for testing equality.
(df + df).equals(df * 2)
    is True

df.sort_index()
    sort rows based on their indexes.

You can conveniently perform element-wise comparisons when comparing a pandas data structure with a scalar value.
df == 0.0
df['A'] == 0.0

Pandas also handles element-wise comparison between different array-like objects of the same size:
pd.Series(['a', 4]) == pd.Index(['a', 3])
pd.Series(['a', 4]) == np.array(['a', 6])
You can not compare two different array-like objects but in numpy you can somehow:
np.array([1,2,3,4]) == np.array([1])
ValueError: np.array([1,2,3,4]) == np.array([1,9])

Combine/Put them together but with prioritized presence of value if it is not NaN:
df1.combine_first(df2)
np.where( CONDITION , IN-CASE-OF-TRUE , IN-CASE-OF-FALSE )
df.combine( df2, lambda x,y: np.where(pd.isna(x), y, x) )

pd.isna( DataFrame or Series )

path = Path('./asd/fgh/sdf/)
if not path.exists():
    path.mkdir()

.mean()
.sum()
.quantile()
.cumsum()
.cumprod()
.std()
df.sum(axis='columns', skipna=True)
    By default
First argument is axis
np.mean(1) , 'rows'
np.mean(0) , 'columns'

df.sub(Series or DataFrame , axis='columns)
df.div

.count, sum, mean, mad, median, min, max, mode, abs, prod, std, var, sem, skew,
kurt, quantile, cumsum, cumprod, cummax, cummin

.nunique()
    series or dataframe will return the number of unique non-nan values.

.idxmin .idxmax
    returnsn the correspondence id of the min/max value.
    in numpy the exact similar methods are `argmin`,`argmax`.

.value_counts()
    on Series and DataFrame
    pd.Series(np.random.randint(0,7, size=50)).value_counts()

Discretization and Quantiling
Continuous values can be descritized using the `cut` (bins based on values)
and `qcut` (bins based on sample quantiles) functions.
    pd.cut(arr, n)
    pd.cut(arr, 3)
    You can provide labels to replace the ranges:
        pd.cut(arr, 3, labels=[1, 2, 3])
You Can Also customize the cut edges by providing an array of edges:
    pd.cut(arr, [...])
    pd.cut(np.arange(20), [1,5,9])
`qcut` computes sample quantiles:
    pd.qcut(np.random.randn(30), [0, .25, .50, .75, 1], labels=range(4))
    pd.qcut(np.random.randn(30), [0, .25, .50, .75, 1], labels=range(4)).value_counts()
    pd.cut( arr , [-np.inf, 0, np.inf], labels=['-', '+'])

pipe()
    Tablewise Function Application.
apply()
    Row or Column-wise Function Application.
agg() and transform()
    Aggregation API.
applymap()
    Applying Elementwise Functions.

df["string-column-name"].str.split(",").str.get(0)

def add_x_column(df, x):
    df[x] = x
    return df
def add_abc_column(df):
    df['abc'] = 'abc'
    return df
df.pipe(add_x_column, 'TT').pipe(add_abc_column)

df.apply(np.mean)
df.apply(np.mean, axis=0)
df.apply(lambda x: x.max() - x.min())
df.apply('mean')
df.apply(lambda x: x.idxmax())
def div(df, by):
    return df / by
df.apply(div, args=(4,))
df.apply(div, by=8)
Interpolate:
    Like fill NaN.
    df.apply(pd.Series.interpolate, method='linear')
    s.interpolate(method='linear')
        [1,2,np.nan,4] -> [1,2,3,4]

DataFrame.aggregation
DataFrame.agg
df.agg(np.sum)
df.agg('sum')
df.sum()
df.sum().sum()
df.agg(['sum'])
df.agg(['sum', 'mean'])
df['col'].agg(['sum', 'mean'])
df.agg(['sum', lambda x: x.mean()])
df.agg({'B': 'mean', 'C': ['sum','mean']})
df.agg({'B': 'mean'})
pd.Series.quantile(df['B'], q=.25)
from functools import partial
q25 = partial(pd.Series.quantile, q=.25)
q25.__name__ = '25%'
df.agg(['std', 'count', 'mean', 'min', 'median', 'max',
        q25])

DataFrame or Series .transform(np.abs)
.transform([np.abs, lambda x: x+1])
    You should see it, it's like each functions applies to each column and puts them along together.
df.transform({'A': np.abs, 'B': [np.abs, lambda x: x+1, 'sqrt']})
So .transform works on just one Series and .apply works on the entire DataFrame at once.

# df.transform(np.sum) --> raises ValueError: Function did not transform
df.apply(np.sum)

Applying elementwise functions:
    Since not all functions can be vectorized (accept NumPy arrays and return
    another array or value), the methods applymap() on DataFrame and analogously
    map() on Series accept any Python function taking a single value and returning a single value.
df.applymap(lambda x: str(x))
df['A'].map(lambda x: str(x))
.map also works on DataFrame too.
s1.map(s2)
    It replaces all values of the s1 with the equivalent values of the s2 series
    where s1 values are equal to s2 labels/indexes and put s2 values in there.

s.reindex([...])
    it's like replace-indexes.
df.reindex(index=[...], columns=[...])
    it's like replace-indexes, replaces-columns.
df.reindex([...], axis='columns/index')
df.reindex_like(df2)
    reindex df like df2, for indexes and columns.

s or df .align(s2)
s or df .align(s2, join='left/right/inner/outer')
s or df .align(s2, join='...', axis='...')

.reindex([...], method='ffill) # forward-fill
    bfill
    nearest
.fillna(method='ffill/bfill)
.interpolate()
    fill linearly.

.reindex([...], method='ffill', limit=3, telorance=1)
    do the method for how many/limit.

.rename(str.upper)
    make indexes uppser case.
.rename(index={'previous':'new'}, columns={'previous':'new'})
.rename({...}, axis='...')
.rename( ... , inplace=True )
s.rename('name')
    altering the Series.name
.rename_axis
    for renaming multi-indexing.

Pandas objects(Series/DataFrame) also have dict-like .items() method.

.itertuples()
df.iterrows()
    gives you the whole dataframe row by row.
    returns a series for each row.

To preserve dtypes while iterating over the rows, it's faster to use .itertuples()
which returns namedtuples of the values and which is generally much faster
than .iterrows(). for instance, a contrived way to transpose the dataframe.

df.T
s.T

for row in df.itertuples():
    row[0] # index-name
    row.col_name
    row[1]

.st and .str accessors.

dts = pd.Series(pd.date_range('2000/1/1', periods=10))
dts.dt.year
dts.dt.tz_localize('US/Eastern')
dts.dt.tz
dts.dt.tz_localize('UTC').dt.tz_convert('US/Eastern')
dts.dt.strftime("%d / %m / %Y")

pd.Series(pd.period_range('2000/1/1', periods=4))
    dtype -> period[D]
pd.Series(pd.date_range('2000/1/1', periods=4, freq='D'))
    dtype -> datatime64[D]

tds.dt.seconds, tds.dt.days
tds.dt.components
    all elements with their intervals like days and hours and  ...

s.str.lower()
        .upper()
        .split()
        .split(' ')
        .split().str.get(0)
        .split(expand=True)
            gives a dataframe.

pandas supports three kinds of sorting: sorting by index labels,
sorting by column values, and sorting by a combination of both.

df.sort_index()
s.sort_index()
.sort_index(axis=0, ascending=True, level='level-name-for-multiIndexing')
.sort_index(key=lambda idx: idx.str.lower())

s.sort_values()
df.sort_values(by='A')
df.sort_values(by=[...])

pd.Series(['a', 'b', 'abc'], dtype='string')
s.sort_values(na_position='first/last')
    define where to put nan values.
df/s.sort_values(key=lambda x: x.str.lower())

idx = pd.MultiIndex.from_tuples([
    ('a', 1), ('a', 2), ('a', 2),
    ('b', 1), ('b', 2), ('b', 1),
])
idx.names = ['first', 'second']
df_multi = pd.DataFrame({'A': np.arange(6,0,-1)}, index=idx)
df_multi
df_multi.sort_values(by=['second', 'A'])

Series.searchsorted is similar to numpy.ndarray.searchsorted.

s.searchsorted([1,2])
    این مقادیر اگر هستن یا اگر میبودن، باید در کدام ایندکس قرا میگرفتند که سورت میبودند

np.random.permutation(5)

.nsmallest(3)
.nlargest(3)
df.nlargest(3, 'Col')
df.nlargest(3, ['Col', 'Col2'])

df.sort_values(by=('a', 'b'))
    sorting a multi-indexed column names.

The copy() method on pandas objects copies the underlying data
(though not the axis indexes, since they are immutable) and returns a new object.

s/df.copy()
    shallow copy.
        it's just a pointer.
s/df.copy(deep=True) # Default
    deep copy.

Important:
As you can see, the new column is not added to the original dataframe as the reference to the dataframe has been copied. In general, a shallow copy allows you
    - Have access to frames data without copying it (memory optimization, etc.)
    - Modify frames structure without reflecting it to the original dataframe
Of course, if you won't create a shallow copy, those changes to dataframe structure will reflect in the original one.
Note that the original data is still shared.

np.may_share_memory(df.A, df2.A)
np.may_share_memory(df, df2)
np.may_share_memory(s, s2)
np.may_share_memory(df.index, df2.index)

DataTypes
    defaults
    upcasting
    astype
    object conversion
    gotchas
    selecting column based on
For the most part, pandas uses NumPy arrays and dtypes for Series or individual columns of a DataFrame.
pandas has two ways to store strings.
    object dtype, which can hold any Python object, including strings.
    StringDtype, which is dedicated to strings.

s/df.astype('...')
pd.Timestamp('20001212')
s.dtype
df.dtypes

The number of columns of each type in a DataFrame can be found by calling DataFrame.dtypes.value_counts().
df.dtypes.value_counts()

.fillna(value=desire-value)

DataFrame.to_numpy()
    will return the lowest-common-denominator of the dtypes.
s/df.to_numpy()
.astype('...')
df.astype({'A': np.int8, 'B': 'float16'})

df.loc[:, ['A', 'B']]

.infer_objects()
    converts dtypes from object to correct dtype.
pd.to_numeric(v, errors='raise/ignore/coerce')
pd.to_datetime(v, errors='raise/ignore/coerce')
pd.to_timedelta(v, errors='raise/ignore/coerce')
pd.to_timedelta(
    ['apple', '2000/01/01', pd.Timedelta('1day')],
    errors='coerce'
)

pd.date_range('now', periods=3)

df.date_column.diff()
    diff gives the datetime difference.

.select_dtypes
df.select_dtypes(include=[np.datetime64], exclude=[bool])
df.select_dtypes(include=['datetime64'], exclude=['bool'])
df.select_dtypes(include=['datetime64'], exclude=['bool', 'number', 'unsignedinteger', 'object']) # 'object' for selecting string values/dtype.

sns.load_dataset('titanic')
df.info()
df.nunique()
{col: df[col].nunique() for col in df if df[col].dtype == object}
df.astype({'sex': 'category'})
df.col.cat.codes
df.col = df.col.cat.codes
    coding categorical dtype columns.

df.corr()
    correlation.
.corr().style.background_gradient(cmap='Greens')
df[['sex', 'alive', 'age']].corr().style.background_gradient(cmap='Greens')
df[['sex', 'alive', 'age']].corr().style.highlight_min()
df[['sex', 'alive', 'age']].corr().style.highlight_min(axis=1)

df.drop(columns='survived', inplace=True)

df.describe()
df.describe(include=['category'])
df.isna()
df.isna().sum()
(~df.isna()).sum()

Number of missing values is too high! 80%! Better to drop.

df.corr()[['age']]
df.corr()[['age']].style.background_gradient('Greens')

.groupby
df.groupby(['adult_male', 'sex']).age.mean()
df.groupby(['adult_male', 'sex']).age.transform('mean')
